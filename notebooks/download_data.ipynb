{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all the csv files from the IABP interpolated data storage website and save them in the data/raw/buoydata folder for later use\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://iabp.apl.uw.edu/Data_Products/Daily_Interp/BuoyData_2024/'\n",
    "\n",
    "# Directory to save the downloaded CSV files\n",
    "output_dir = '../data/raw/buoydata/past'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all links ending with .csv\n",
    "csv_links = soup.find_all('a', href=lambda href: href and href.endswith('.csv'))\n",
    "\n",
    "# Download each CSV file\n",
    "for link in csv_links:\n",
    "    csv_url = urljoin(url, link['href'])\n",
    "    csv_response = requests.get(csv_url)\n",
    "    csv_response.raise_for_status()\n",
    "    \n",
    "    # Extract the filename from the URL\n",
    "    filename = os.path.join(output_dir, os.path.basename(csv_url))\n",
    "    \n",
    "    # Save the CSV file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(csv_response.content)\n",
    "    \n",
    "    print(f'Downloaded {filename}')\n",
    "\n",
    "print('All files downloaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the last n days of buoy data for use in predictions with IDs of your choice\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# List of bouyID values\n",
    "bids = [\n",
    "    '300234011751690',\n",
    "    #'300234011751691',\n",
    "    #'300234011751692'\n",
    "    # Add more bid values as needed\n",
    "]\n",
    "\n",
    "# Directory to save the downloaded CSV files\n",
    "output_dir = '../data/raw/buoydata/current'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Number of days to download data for\n",
    "ndays = 10\n",
    "\n",
    "# Base URL for the API\n",
    "base_url = 'https://iabp.apl.uw.edu/download'\n",
    "\n",
    "# Iterate over each bid value\n",
    "for bid in bids:\n",
    "    # Construct the URL for the current bid\n",
    "    url = f'{base_url}?bid={bid}&ndays={ndays}'\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    \n",
    "    # Construct the filename and save path\n",
    "    filename = f'{bid}.csv'\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Save the CSV file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    print(f'Downloaded {filename} to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "import os\n",
    "\n",
    "# FTP server details\n",
    "ftp_server = 'ftp.cdc.noaa.gov'\n",
    "ftp_path = '/Datasets/ncep/'\n",
    "filename = 'uwnd.2024.nc'\n",
    "\n",
    "# Local directory to save the downloaded file\n",
    "local_dir = '../data/raw/reanalyses/ncep'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Connect to the FTP server\n",
    "ftp = ftplib.FTP(ftp_server)\n",
    "ftp.login()\n",
    "\n",
    "# Change to the specified directory\n",
    "ftp.cwd(ftp_path)\n",
    "\n",
    "# Download the file\n",
    "local_filename = os.path.join(local_dir, filename)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    ftp.retrbinary(f'RETR {filename}', file.write)\n",
    "\n",
    "print(f'Downloaded {filename} to {local_filename}')\n",
    "\n",
    "#Doing the same for the vwnd file\n",
    "\n",
    "filename = 'vwnd.2024.nc'\n",
    "\n",
    "# Local directory to save the downloaded file\n",
    "local_dir = '../data/raw/reanalyses/ncep'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Connect to the FTP server\n",
    "ftp = ftplib.FTP(ftp_server)\n",
    "ftp.login()\n",
    "\n",
    "# Change to the specified directory\n",
    "ftp.cwd(ftp_path)\n",
    "\n",
    "# Download the file\n",
    "local_filename = os.path.join(local_dir, filename)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    ftp.retrbinary(f'RETR {filename}', file.write)\n",
    "\n",
    "print(f'Downloaded {filename} to {local_filename}')\n",
    "\n",
    "# Close the FTP connection\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the most recent GFS forecast data from the NOAA FTP server\n",
    "\n",
    "import ftplib\n",
    "from datetime import datetime\n",
    "import os\n",
    "from ftplib import FTP\n",
    "\n",
    "# FTP server details\n",
    "ftp_server = 'ftp.ncep.noaa.gov'\n",
    "ftp_path = '/pub/data/nccf/com/gfs/prod/'\n",
    "\n",
    "# Connect to the FTP server\n",
    "ftp = ftplib.FTP(ftp_server)\n",
    "ftp.login()\n",
    "\n",
    "# Change to the specified directory\n",
    "ftp.cwd(ftp_path)\n",
    "\n",
    "# List directories and their modification times\n",
    "directories = []\n",
    "ftp.retrlines('LIST', directories.append)\n",
    "\n",
    "# Filter directories with \"gfs\" in their name and get their modification times\n",
    "gfs_dirs = []\n",
    "for entry in directories:\n",
    "    parts = entry.split()\n",
    "    name = parts[-1]\n",
    "    if 'gfs' in name:\n",
    "        # Parse the modification time\n",
    "        mod_time_str = ' '.join(parts[-4:-1])\n",
    "        mod_time = datetime.strptime(mod_time_str, '%b %d %H:%M')\n",
    "        gfs_dirs.append((name, mod_time))\n",
    "\n",
    "# Find the most recently edited directory\n",
    "most_recent_dir = max(gfs_dirs, key=lambda x: x[1])\n",
    "\n",
    "# Enter the most recently edited directory\n",
    "ftp.cwd(most_recent_dir[0])\n",
    "print(f\"Entered directory: {most_recent_dir[0]}\")\n",
    "\n",
    "# List directories and their modification times in the current directory\n",
    "subdirectories = []\n",
    "ftp.retrlines('LIST', subdirectories.append)\n",
    "\n",
    "# Filter directories and get their modification times\n",
    "sub_dirs = []\n",
    "for entry in subdirectories:\n",
    "    parts = entry.split()\n",
    "    name = parts[-1]\n",
    "    if entry.startswith('d'):  # Check if it's a directory\n",
    "        # Parse the modification time\n",
    "        mod_time_str = ' '.join(parts[-4:-1])\n",
    "        mod_time = datetime.strptime(mod_time_str, '%b %d %H:%M')\n",
    "        sub_dirs.append((name, mod_time))\n",
    "\n",
    "# Find the most recently edited subdirectory\n",
    "most_recent_subdir = max(sub_dirs, key=lambda x: x[1])\n",
    "\n",
    "# Enter the most recently edited subdirectory\n",
    "ftp.cwd(most_recent_subdir[0])\n",
    "print(f\"Entered subdirectory: {most_recent_subdir[0]}\")\n",
    "\n",
    "# Enter the \"atmos\" folder\n",
    "ftp.cwd('atmos')\n",
    "print(\"Entered 'atmos' folder\")\n",
    "\n",
    "# List files in the \"atmos\" folder\n",
    "files = []\n",
    "ftp.retrlines('LIST', files.append)\n",
    "\n",
    "# Filter .nc files and get their modification times\n",
    "nc_files = []\n",
    "for entry in files:\n",
    "    parts = entry.split()\n",
    "    name = parts[-1]\n",
    "    if name.endswith('.nc'):\n",
    "        # Parse the modification time\n",
    "        mod_time_str = ' '.join(parts[-4:-1])\n",
    "        mod_time = datetime.strptime(mod_time_str, '%b %d %H:%M')\n",
    "        nc_files.append((name, mod_time))\n",
    "\n",
    "# Find the most recently edited .nc file\n",
    "most_recent_nc_file = max(nc_files, key=lambda x: x[1])\n",
    "\n",
    "# Ensure the local directory exists\n",
    "local_dir = '../data/raw/forecasts/gfs'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Download the most recently edited .nc file\n",
    "local_filename = os.path.join(local_dir, most_recent_nc_file[0])\n",
    "with open(local_filename, 'wb') as file:\n",
    "    ftp.retrbinary(f'RETR {most_recent_nc_file[0]}', file.write)\n",
    "\n",
    "print(f'Downloaded {most_recent_nc_file[0]} to {local_filename}')\n",
    "\n",
    "# Close the FTP connection\n",
    "ftp.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
