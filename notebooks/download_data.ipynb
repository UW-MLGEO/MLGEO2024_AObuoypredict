{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downloading\n",
    "## 1. Download a collection of past buoy transmissions from the IABP website (with interpolated MERRA-2 data)\n",
    "This section will download a collection of past buoy transmissions from a (hidden) section of the IABP website where buoy data were interpolated with MERRA-2 reanalysis data for use in another project. We will make use of that data here for use in training machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all the csv files from the IABP interpolated data storage website and save them in the data/raw/buoydata folder for later use\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://iabp.apl.uw.edu/Data_Products/Daily_Interp/BuoyData_2024/'\n",
    "\n",
    "# Directory to save the downloaded CSV files\n",
    "output_dir = '../data/raw/buoydata/past'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all links ending with .csv\n",
    "csv_links = soup.find_all('a', href=lambda href: href and href.endswith('.csv'))\n",
    "\n",
    "# Download each CSV file\n",
    "for link in csv_links:\n",
    "    csv_url = urljoin(url, link['href'])\n",
    "    csv_response = requests.get(csv_url)\n",
    "    csv_response.raise_for_status()\n",
    "    \n",
    "    # Extract the filename from the URL\n",
    "    filename = os.path.join(output_dir, os.path.basename(csv_url))\n",
    "    \n",
    "    # Save the CSV file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(csv_response.content)\n",
    "    \n",
    "    print(f'Downloaded {filename}')\n",
    "\n",
    "print('All files downloaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download a collection of real-time buoy data for use in predictions\n",
    "This section will download real-time buoy data from the IABP website. All buoys that have reported in the last 24 hours will be queried and downloaded. Sometimes server errors can occur with the API so those that produce a 500 error will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the last n days of buoy data (you can change below) for use in predictions with IDs of your choice\n",
    "# The data will be saved in the data/raw/buoydata/current folder. Note that buoys that produce a 500 error will be skipped.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# URL to get the table of all buoys\n",
    "table_url = 'https://iabp.apl.uw.edu/TABLES/ArcticTable_Current.txt'\n",
    "\n",
    "# Fetch the table\n",
    "response = requests.get(table_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the table to a DataFrame without a header\n",
    "data = response.text.splitlines()\n",
    "rows = [line.split(';') for line in data]\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Manually select the first column (buoy ID) and the seventh column (date)\n",
    "df = df[[0, 6]]\n",
    "df.columns = ['BuoyID', 'Date']\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now(timezone.utc)\n",
    "print(f\"Current time: {current_time}\")\n",
    "\n",
    "# Filter buoy IDs that have reported in the last 24 hours\n",
    "bids = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Update the date format to match MM/DD/YYYY HH:MM:SS\n",
    "        report_time = datetime.strptime(row['Date'], '%m/%d/%Y %H:%M:%S').replace(tzinfo=timezone.utc)\n",
    "        if current_time - report_time <= timedelta(hours=24):\n",
    "            bids.append(row['BuoyID'])\n",
    "    except ValueError as e:\n",
    "        continue  # Skip rows with invalid date format\n",
    "\n",
    "print(f'Selected buoy IDs: {bids}')\n",
    "\n",
    "# Directory to save the downloaded CSV files\n",
    "output_dir = '../data/raw/buoydata/current'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear all files in the directory before downloading new data\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            os.rmdir(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# Number of days to download data for\n",
    "ndays = 2\n",
    "\n",
    "# Base URL for the API\n",
    "base_url = 'https://iabp.apl.uw.edu/download'\n",
    "\n",
    "# Iterate over each bid value\n",
    "for bid in bids:\n",
    "    # Construct the URL for the current bid\n",
    "    url = f'{base_url}?bid={bid}&ndays={ndays}'\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Construct the filename and save path\n",
    "        filename = f'{bid}.csv'\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the CSV file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \n",
    "        print(f'Downloaded {filename} to {file_path}')\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 500:\n",
    "            print(f\"Skipping {bid} due to HTTP 500 error\")\n",
    "        else:\n",
    "            print(f\"HTTP error occurred for {bid}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {bid}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download NCEP surface wind (u and v) products for interpolation with past buoy data\n",
    "This section will download the 2024 NCEP reanalysis (u and v components of wind) and save as a netCDF. This data will be interpolated with the past buoy data as training data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the NCEP surface winds (u and v) reanalysis data for the year 2024\n",
    "#The data is stored in NetCDF format on an FTP server. We will download the files and save them in the data/raw/reanalyses/ncep folder.\n",
    "\n",
    "import ftplib\n",
    "import os\n",
    "\n",
    "# FTP server details\n",
    "ftp_server = 'ftp.cdc.noaa.gov'\n",
    "ftp_path = '/Datasets/ncep/'\n",
    "filename = 'uwnd.sfc.2024.nc'\n",
    "\n",
    "# Local directory to save the downloaded file\n",
    "local_dir = '../data/raw/reanalyses/ncep'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Connect to the FTP server\n",
    "ftp = ftplib.FTP(ftp_server)\n",
    "ftp.login()\n",
    "\n",
    "# Change to the specified directory\n",
    "ftp.cwd(ftp_path)\n",
    "\n",
    "# Download the file\n",
    "local_filename = os.path.join(local_dir, filename)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    ftp.retrbinary(f'RETR {filename}', file.write)\n",
    "\n",
    "print(f'Downloaded {filename} to {local_filename}')\n",
    "\n",
    "#Doing the same for the vwnd file\n",
    "\n",
    "filename = 'vwnd.sfc.2024.nc'\n",
    "\n",
    "# Local directory to save the downloaded file\n",
    "local_dir = '../data/raw/reanalyses/ncep'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Connect to the FTP server\n",
    "ftp = ftplib.FTP(ftp_server)\n",
    "ftp.login()\n",
    "\n",
    "# Change to the specified directory\n",
    "ftp.cwd(ftp_path)\n",
    "\n",
    "# Download the file\n",
    "local_filename = os.path.join(local_dir, filename)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    ftp.retrbinary(f'RETR {filename}', file.write)\n",
    "\n",
    "print(f'Downloaded {filename} to {local_filename}')\n",
    "\n",
    "# Close the FTP connection\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download the most recent GFS forecast to interpolate with the real-time buoy data\n",
    "This section will access the NOAA FTP repository and download the most recent GFS forecast data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the most recent GFS forecast data from the NOAA FTP server\n",
    "# The data is stored in netCDF format in the data/raw/forecasts/gfs directory\n",
    "# Be aware that the file is large and may take some time to download and that all files in the gfs directory will be wiped before downloading the new data\n",
    "\n",
    "import ftplib\n",
    "from datetime import datetime\n",
    "import os\n",
    "from ftplib import FTP\n",
    "\n",
    "# Define the directory path to the gfs folder\n",
    "gfs_directory = 'data/raw/forecasts/gfs'\n",
    "\n",
    "# Remove all files in the gfs directory before downloading new data\n",
    "for filename in os.listdir(gfs_directory):\n",
    "    file_path = os.path.join(gfs_directory, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            os.rmdir(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# FTP server details\n",
    "ftp_server = 'ftp.ncep.noaa.gov'\n",
    "ftp_path = '/pub/data/nccf/com/gfs/prod/'\n",
    "\n",
    "# Connect to the FTP server\n",
    "ftp = ftplib.FTP(ftp_server)\n",
    "ftp.login()\n",
    "\n",
    "# Change to the specified directory\n",
    "ftp.cwd(ftp_path)\n",
    "\n",
    "# List directories and their modification times\n",
    "directories = []\n",
    "ftp.retrlines('LIST', directories.append)\n",
    "\n",
    "# Filter directories with \"gfs\" in their name and get their modification times\n",
    "gfs_dirs = []\n",
    "for entry in directories:\n",
    "    parts = entry.split()\n",
    "    name = parts[-1]\n",
    "    if 'gfs' in name:\n",
    "        # Parse the modification time\n",
    "        mod_time_str = ' '.join(parts[-4:-1])\n",
    "        mod_time = datetime.strptime(mod_time_str, '%b %d %H:%M')\n",
    "        gfs_dirs.append((name, mod_time))\n",
    "\n",
    "# Find the most recently edited directory\n",
    "most_recent_dir = max(gfs_dirs, key=lambda x: x[1])\n",
    "\n",
    "# Enter the most recently edited directory\n",
    "ftp.cwd(most_recent_dir[0])\n",
    "print(f\"Entered directory: {most_recent_dir[0]}\")\n",
    "\n",
    "# List subdirectories and their modification times\n",
    "subdirectories = []\n",
    "ftp.retrlines('LIST', subdirectories.append)\n",
    "\n",
    "# Filter subdirectories and get their modification times\n",
    "sub_dirs = []\n",
    "for entry in subdirectories:\n",
    "    parts = entry.split()\n",
    "    name = parts[-1]\n",
    "    if entry.startswith('d'):\n",
    "        # Parse the modification time\n",
    "        mod_time_str = ' '.join(parts[-4:-1])\n",
    "        mod_time = datetime.strptime(mod_time_str, '%b %d %H:%M')\n",
    "        sub_dirs.append((name, mod_time))\n",
    "\n",
    "# Find the most recently edited subdirectory\n",
    "most_recent_subdir = max(sub_dirs, key=lambda x: x[1])\n",
    "\n",
    "# Enter the most recently edited subdirectory\n",
    "ftp.cwd(most_recent_subdir[0])\n",
    "print(f\"Entered subdirectory: {most_recent_subdir[0]}\")\n",
    "\n",
    "# Enter the \"atmos\" directory\n",
    "ftp.cwd('atmos')\n",
    "print(\"Entered directory: atmos\")\n",
    "\n",
    "# List files in the directory\n",
    "files = []\n",
    "ftp.retrlines('LIST', files.append)\n",
    "\n",
    "# Filter .nc files with \"atmf\" in their name and get their modification times\n",
    "nc_files = []\n",
    "for entry in files:\n",
    "    parts = entry.split()\n",
    "    name = parts[-1]\n",
    "    if name.endswith('.nc') and 'atmf' in name:\n",
    "        # Parse the modification time\n",
    "        mod_time_str = ' '.join(parts[-4:-1])\n",
    "        mod_time = datetime.strptime(mod_time_str, '%b %d %H:%M')\n",
    "        nc_files.append((name, mod_time))\n",
    "\n",
    "# Find the most recently edited .nc file with \"atmf\" in its name\n",
    "most_recent_nc_file = max(nc_files, key=lambda x: x[1])\n",
    "\n",
    "# Ensure the local directory exists\n",
    "local_dir = '../data/raw/forecasts/gfs'\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Download the most recently edited .nc file\n",
    "local_filename = os.path.join(local_dir, most_recent_nc_file[0])\n",
    "with open(local_filename, 'wb') as file:\n",
    "    ftp.retrbinary(f'RETR {most_recent_nc_file[0]}', file.write)\n",
    "\n",
    "print(f'Downloaded {most_recent_nc_file[0]} to {local_filename}')\n",
    "\n",
    "# Close the FTP connection\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download a raster dataset of the land in our area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u6FwmRsOH7FfY7jhb0XQJ6yurWwy3AOW\n",
      "To: c:\\Users\\benco\\Documents\\GitHub\\MLGEO2024_AObuoypredict\\data\\raw\\geospatial\\arctic_land.tif\n",
      "100%|██████████| 5.15M/5.15M [00:00<00:00, 24.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file to ../data/raw/geospatial\\arctic_land.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "\n",
    "# URL of the Google Drive file\n",
    "url = 'https://drive.google.com/uc?id=1u6FwmRsOH7FfY7jhb0XQJ6yurWwy3AOW'\n",
    "\n",
    "# Directory to save the downloaded file\n",
    "output_dir = '../data/raw/geospatial'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Output file path\n",
    "output_file = os.path.join(output_dir, 'arctic_land.tif')\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output_file, quiet=False)\n",
    "\n",
    "print(f'Downloaded file to {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
