{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downloading\n",
    "## 1. Download a collection of past buoy transmissions from the IABP website\n",
    "This section will download a collection of past buoy transmissions from a section of the IABP website. We will make use of that data here for use in training machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# URL of the file to download\n",
    "file_url = 'https://iabp.apl.uw.edu/Data_Products/LEVEL1_DATA/LEVEL1_2023.csv'\n",
    "\n",
    "# Directory to save the downloaded file\n",
    "temp_dir = '../data/raw/buoydata/past/temp'\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Path to save the downloaded file\n",
    "temp_file_path = os.path.join(temp_dir, 'IABP_Level1_2023all.csv')\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(file_url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Save the file\n",
    "with open(temp_file_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(f'Downloaded file to {temp_file_path}')\n",
    "\n",
    "# Read the downloaded CSV file\n",
    "df = pd.read_csv(temp_file_path)\n",
    "\n",
    "# Directory to save the separated CSV files\n",
    "output_dir = '../data/raw/buoydata/past'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear all files in the directory before saving new data, except the \"temp\" folder\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path) and filename != 'temp':\n",
    "            os.rmdir(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "    \n",
    "# Separate the file into discrete CSV files based on the buoyID column\n",
    "for buoy_id, group in df.groupby('BuoyID'):\n",
    "    output_file_path = os.path.join(output_dir, f'{buoy_id}.csv')\n",
    "    group.to_csv(output_file_path, index=False)\n",
    "    print(f'Saved {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download a collection of real-time buoy data for use in predictions\n",
    "This section will download real-time buoy data from the IABP website. All buoys that have reported in the last 24 hours will be queried and downloaded. Sometimes server errors can occur with the API so those that produce a 500 error will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the last n days of buoy data (you can change below) for use in predictions with IDs of your choice\n",
    "# The data will be saved in the data/raw/buoydata/current folder. Note that buoys that produce a 500 error will be skipped.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# URL to get the table of all buoys\n",
    "table_url = 'https://iabp.apl.uw.edu/TABLES/ArcticTable_Current.txt'\n",
    "\n",
    "# Fetch the table\n",
    "response = requests.get(table_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Convert the table to a DataFrame without a header\n",
    "data = response.text.splitlines()\n",
    "rows = [line.split(';') for line in data]\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Manually select the first column (buoy ID) and the seventh column (date)\n",
    "df = df[[0, 6]]\n",
    "df.columns = ['BuoyID', 'Date']\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now(timezone.utc)\n",
    "print(f\"Current time: {current_time}\")\n",
    "\n",
    "# Filter buoy IDs that have reported in the last 24 hours\n",
    "bids = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Update the date format to match MM/DD/YYYY HH:MM:SS\n",
    "        report_time = datetime.strptime(row['Date'], '%m/%d/%Y %H:%M:%S').replace(tzinfo=timezone.utc)\n",
    "        if current_time - report_time <= timedelta(hours=24):\n",
    "            bids.append(row['BuoyID'])\n",
    "    except ValueError as e:\n",
    "        continue  # Skip rows with invalid date format\n",
    "\n",
    "print(f'Selected buoy IDs: {bids}')\n",
    "\n",
    "# Directory to save the downloaded CSV files\n",
    "output_dir = '../data/raw/buoydata/current'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Clear all files in the directory before downloading new data\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            os.rmdir(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# Number of days to download data for\n",
    "ndays = 2\n",
    "\n",
    "# Base URL for the API\n",
    "base_url = 'https://iabp.apl.uw.edu/download'\n",
    "\n",
    "# Iterate over each bid value\n",
    "for bid in bids:\n",
    "    # Construct the URL for the current bid\n",
    "    url = f'{base_url}?bid={bid}&ndays={ndays}'\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Construct the filename and save path\n",
    "        filename = f'{bid}.csv'\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the CSV file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \n",
    "        print(f'Downloaded {filename} to {file_path}')\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 500:\n",
    "            print(f\"Skipping {bid} due to HTTP 500 error\")\n",
    "        else:\n",
    "            print(f\"HTTP error occurred for {bid}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {bid}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download ERA5 surface wind (u and v) products for interpolation with past buoy data\n",
    "This section will download the 2023 ERA5 reanalysis (u and v components of wind) and save as a netCDF. This data will be interpolated with the past buoy data as training data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:32:11,048 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-07 15:32:11,049 WARNING [2024-09-26T00:00:00] Should you have not yet migrated from the old CDS system to the new CDS, please check our [informative page](https://confluence.ecmwf.int/x/uINmFw) for guidance.\n",
      "2024-11-07 15:32:11,049 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-07 15:32:11,050 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-07 15:32:11,050 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-07 15:32:11,510 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-07 15:32:11,511 INFO Request ID is cafaf517-7c79-4f55-bc87-6d94b36d29b0\n",
      "2024-11-07 15:32:11,725 INFO status has been updated to accepted\n",
      "2024-11-07 15:32:14,624 INFO status has been updated to running\n",
      "2024-11-07 15:32:17,066 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9591d666f2a4b6191547579cddf97b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "f490db7cc025062d24651ab80175da7f.nc:   0%|          | 0.00/425M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:34:14,677 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-07 15:34:14,678 WARNING [2024-09-26T00:00:00] Should you have not yet migrated from the old CDS system to the new CDS, please check our [informative page](https://confluence.ecmwf.int/x/uINmFw) for guidance.\n",
      "2024-11-07 15:34:14,679 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-07 15:34:14,679 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-07 15:34:14,680 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-07 15:34:15,269 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-07 15:34:15,270 INFO Request ID is d33ed661-3f3d-4163-90ce-31bcffeb5d6f\n",
      "2024-11-07 15:34:15,454 INFO status has been updated to accepted\n",
      "2024-11-07 15:34:20,792 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da2af2919404590b8315c06f7e609a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1b4bc0e1230e9c9c199946081909c444.nc:   0%|          | 0.00/446M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script will download the ERA5 reanalysis data for the year 2023 from the CDS API. The data will be downloaded in netCDF format and will contain the u-component of the wind at the 1 hPa pressure level. The data will be downloaded for the entire globe and for all hours of the day. The data will be saved in the data/raw/reanalyses/ERA5 directory. \n",
    "# If the directory does not exist, it will be created. If the directory already exists, all files in the directory will be deleted before the new data is downloaded.\n",
    "# WARNING: the output file will be large (approximately 1.5 GB) and the download may take a long time. Make sure you have enough disk space and a stable internet connection before running this script.\n",
    "\n",
    "import os\n",
    "import cdsapi\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(\"../data/raw/reanalyses/ERA5\", exist_ok=True)\n",
    "        \n",
    "# Set the CDSAPI_RC environment variable to the path of your .cdsapirc file\n",
    "os.environ['CDSAPI_RC'] = '../.cdsapirc'\n",
    "\n",
    "dataset = \"reanalysis-era5-pressure-levels\"\n",
    "request = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"variable\": [\"u_component_of_wind\"],\n",
    "    \"year\": [\"2023\"],\n",
    "    \"month\": [\n",
    "        \"01\", #\"02\", \"03\",\n",
    "        #\"04\", \"05\", \"06\",\n",
    "        #\"07\", \"08\", \"09\",\n",
    "        #\"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"day\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\",\n",
    "        \"13\", \"14\", \"15\",\n",
    "        \"16\", \"17\", \"18\",\n",
    "        \"19\", \"20\", \"21\",\n",
    "        \"22\", \"23\", \"24\",\n",
    "        \"25\", \"26\", \"27\",\n",
    "        \"28\", \"29\", \"30\",\n",
    "        \"31\"\n",
    "    ],\n",
    "    \"time\": [\n",
    "        \"00:00\", \"01:00\", \"02:00\",\n",
    "        \"03:00\", \"04:00\", \"05:00\",\n",
    "        \"06:00\", \"07:00\", \"08:00\",\n",
    "        \"09:00\", \"10:00\", \"11:00\",\n",
    "        \"12:00\", \"13:00\", \"14:00\",\n",
    "        \"15:00\", \"16:00\", \"17:00\",\n",
    "        \"18:00\", \"19:00\", \"20:00\",\n",
    "        \"21:00\", \"22:00\", \"23:00\"\n",
    "    ],\n",
    "    \"pressure_level\": [\"1\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "    \"area\": [90, -180, 23, 180]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\n",
    "target_file = \"../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc\"\n",
    "client.retrieve(dataset, request, target_file)\n",
    "\n",
    "dataset = \"reanalysis-era5-pressure-levels\"\n",
    "request = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"variable\": [\"v_component_of_wind\"],\n",
    "    \"year\": [\"2023\"],\n",
    "    \"month\": [\n",
    "        \"01\", #\"02\", \"03\",\n",
    "        #\"04\", \"05\", \"06\",\n",
    "        #\"07\", \"08\", \"09\",\n",
    "        #\"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"day\": [\n",
    "        \"01\", \"02\", \"03\",\n",
    "        \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\",\n",
    "        \"10\", \"11\", \"12\",\n",
    "        \"13\", \"14\", \"15\",\n",
    "        \"16\", \"17\", \"18\",\n",
    "        \"19\", \"20\", \"21\",\n",
    "        \"22\", \"23\", \"24\",\n",
    "        \"25\", \"26\", \"27\",\n",
    "        \"28\", \"29\", \"30\",\n",
    "        \"31\"\n",
    "    ],\n",
    "    \"time\": [\n",
    "        \"00:00\", \"01:00\", \"02:00\",\n",
    "        \"03:00\", \"04:00\", \"05:00\",\n",
    "        \"06:00\", \"07:00\", \"08:00\",\n",
    "        \"09:00\", \"10:00\", \"11:00\",\n",
    "        \"12:00\", \"13:00\", \"14:00\",\n",
    "        \"15:00\", \"16:00\", \"17:00\",\n",
    "        \"18:00\", \"19:00\", \"20:00\",\n",
    "        \"21:00\", \"22:00\", \"23:00\"\n",
    "    ],\n",
    "    \"pressure_level\": [\"1\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "    \"area\": [90, -180, 23, 180]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\n",
    "target_file = \"../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc\"\n",
    "client.retrieve(dataset, request, target_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
