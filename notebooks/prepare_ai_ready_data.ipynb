{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing past buoy data and reanalyses for use in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenate all past buoy data into a single dataframe\n",
    "This section will collect all of the cleaned buoy data and combine them into a single dataframe. A column to represent the day of year (DOY) as an integer is also added. These data will be used (along with weather reanalyses) as training data for the machine learning model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multiple raw buoy CSV files into a single DataFrame and add a new column with the Day of Year (DOY) as an integer\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = '../data/cleaned/buoydata/past'\n",
    "\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the list of CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename the lat and lon columns to Latitude and Longitude\n",
    "combined_df.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Pad Month, Day, Hour, Min, and Sec columns with leading zeros\n",
    "combined_df['Month'] = combined_df['Month'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Day'] = combined_df['Day'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Hour'] = combined_df['Hour'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Min'] = combined_df['Min'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Sec'] = combined_df['Sec'].apply(lambda x: f'{x:02}')\n",
    "\n",
    "# Create a new column called datetime by combining Year, Month, Day, Hour, Min, and Sec columns\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['Year'].astype(str) + '-' +\n",
    "                                         combined_df['Month'].astype(str) + '-' +\n",
    "                                         combined_df['Day'].astype(str) + ' ' +\n",
    "                                         combined_df['Hour'].astype(str) + ':' +\n",
    "                                         combined_df['Min'].astype(str) + ':' +\n",
    "                                         combined_df['Sec'].astype(str))\n",
    "\n",
    "# Add a new column with the Day of Year (DOY) as an integer\n",
    "combined_df['DOY'] = combined_df['datetime'].dt.dayofyear\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_buoy_data.csv', index=False)\n",
    "print(\"combined_df has been saved to 'combined_buoy_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more data to the spreadsheet (wind vector and displacement/heading columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "from geopy import Point\n",
    "import math\n",
    "\n",
    "# Load and Preprocess Data\n",
    "\n",
    "print(\"Loading buoy data from the spreadsheet...\")\n",
    "\n",
    "# Load the buoy data from the spreadsheet\n",
    "buoy_data = pd.read_csv('combined_buoy_data.csv')\n",
    "print(\"Buoy data loaded successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "print(\"Extracting necessary columns...\")\n",
    "\n",
    "# Extract necessary columns\n",
    "buoy_data = buoy_data[['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd']]\n",
    "print(\"Columns extracted successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "print(\"Rounding wind columns to two decimal places...\")\n",
    "\n",
    "# Round wind columns to two decimal places\n",
    "buoy_data['era5_uwnd'] = buoy_data['era5_uwnd'].round(2)\n",
    "buoy_data['era5_vwnd'] = buoy_data['era5_vwnd'].round(2)\n",
    "print(\"Wind columns rounded successfully.\")\n",
    "\n",
    "print(\"Calculating wind magnitude and wind angle...\")\n",
    "\n",
    "# Calculate wind magnitude and wind angle\n",
    "buoy_data['wind_magnitude'] = np.sqrt(buoy_data['era5_uwnd']**2 + buoy_data['era5_vwnd']**2)\n",
    "buoy_data['wind_angle'] = np.degrees(np.arctan2(buoy_data['era5_vwnd'], buoy_data['era5_uwnd']))\n",
    "\n",
    "print(\"Wind magnitude and wind angle calculated successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "print(\"Displaying the first few rows of the preprocessed data:\")\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "buoy_data.head()\n",
    "\n",
    "print(\"Calculating displacement and heading...\")\n",
    "\n",
    "# Initialize displacement and heading columns\n",
    "buoy_data['displacement'] = 0.0\n",
    "buoy_data['heading'] = 0.0\n",
    "\n",
    "# Function to calculate displacement and heading for each group\n",
    "def calculate_displacement_and_heading(group):\n",
    "    group = group.sort_values(by='datetime').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        prev_point = Point(group.loc[i-1, 'Latitude'], group.loc[i-1, 'Longitude'])\n",
    "        curr_point = Point(group.loc[i, 'Latitude'], group.loc[i, 'Longitude'])\n",
    "        \n",
    "        # Calculate displacement\n",
    "        group.loc[i, 'displacement'] = great_circle(prev_point, curr_point).meters\n",
    "        \n",
    "        # Calculate heading\n",
    "        lat1, lon1 = math.radians(group.loc[i-1, 'Latitude']), math.radians(group.loc[i-1, 'Longitude'])\n",
    "        lat2, lon2 = math.radians(group.loc[i, 'Latitude']), math.radians(group.loc[i, 'Longitude'])\n",
    "        \n",
    "        dlon = lon2 - lon1\n",
    "        x = math.sin(dlon) * math.cos(lat2)\n",
    "        y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(dlon))\n",
    "        initial_heading = math.atan2(x, y)\n",
    "        initial_heading = math.degrees(initial_heading)\n",
    "        compass_heading = (initial_heading + 360) % 360\n",
    "        \n",
    "        group.loc[i, 'heading'] = compass_heading\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "buoy_data = buoy_data.groupby('BuoyID').apply(calculate_displacement_and_heading).reset_index(drop=True)\n",
    "\n",
    "print(\"Displacement and heading calculated successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "# Save the processed buoy_data back to the spreadsheet\n",
    "output_csv_path = 'processed_buoy_data.csv'\n",
    "buoy_data.to_csv(output_csv_path, index=False)\n",
    "print(f\"Processed buoy data saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaned buoy data geospatial bounds confirmation\n",
    "This cell will analyze and display the minimum and maximum values of the latitude and longitude fields of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the latitude and longitude ranges\n",
    "\n",
    "min_latitude = combined_df['Latitude'].min()\n",
    "max_latitude = combined_df['Latitude'].max()\n",
    "min_longitude = combined_df['Longitude'].min()\n",
    "max_longitude = combined_df['Longitude'].max()\n",
    "\n",
    "print(f\"Latitude: min = {min_latitude}, max = {max_latitude}\")\n",
    "print(f\"Longitude: min = {min_longitude}, max = {max_longitude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interpolate ERA5 reanalysis data to past buoy locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load the NetCDF files\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "# Extract the valid_time, latitudes, longitudes, and u-component wind values from the NetCDF file\n",
    "valid_time = uwnd_ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "latitudes = uwnd_ds.variables['latitude'][:]\n",
    "longitudes = uwnd_ds.variables['longitude'][:]\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Add a column to the dataframe called \"timestamp\"\n",
    "combined_df['timestamp'] = combined_df['datetime'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "\n",
    "# Create a KDTree for fast spatial lookup\n",
    "lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "# Add new columns to combined_df for the u-component and v-component wind values\n",
    "combined_df['era5_uwnd'] = np.nan\n",
    "combined_df['era5_vwnd'] = np.nan\n",
    "\n",
    "# Check the shape of the uwnd_array\n",
    "print(f\"uwnd_array shape: {uwnd_array.shape}\")\n",
    "print(f\"vwnd_array shape: {vwnd_array.shape}\")\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Find the value of the netCDF variable valid_time closest to the timestamp value\n",
    "    timestamp = row['timestamp']\n",
    "    time_diffs = np.abs(valid_time - timestamp)\n",
    "    closest_time_index = np.argmin(time_diffs)\n",
    "    \n",
    "    # Check if the calculated index is within the bounds of the uwnd_array\n",
    "    if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "        print(f\"Skipping row {index} with timestamp {timestamp} as it is out of bounds\")\n",
    "        continue\n",
    "    \n",
    "    # Select the corresponding netCDF slices\n",
    "    uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "    vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "    \n",
    "    # Find the grid cell of the netCDF slice closest to the Latitude and Longitude position\n",
    "    lat_lon = (row['Latitude'], row['Longitude'])\n",
    "    _, closest_point_index = tree.query(lat_lon)\n",
    "    closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "    \n",
    "    # Find the index of the closest latitude/longitude pair in the arrays\n",
    "    lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "    lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "    \n",
    "    # Assign the corresponding u and v values to the new columns in the dataframe\n",
    "    combined_df.at[index, 'era5_uwnd'] = uwnd_slice[lat_index, lon_index]\n",
    "    combined_df.at[index, 'era5_vwnd'] = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "# Drop the timestamp column from the dataframe\n",
    "combined_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Print the dataframe head\n",
    "print(combined_df.head())\n",
    "\n",
    "# Print a message saying the script has completed\n",
    "print(\"The script has completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
