{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing past buoy data and reanalyses for use in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenate all past buoy data into a single dataframe\n",
    "This section will collect all of the cleaned buoy data and combine them into a single dataframe. A column to represent the day of year (DOY) as an integer is also added. These data will be used (along with weather reanalyses) as training data for the machine learning model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multiple raw buoy CSV files into a single DataFrame and add a new column with the Day of Year (DOY) as an integer\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = '../data/cleaned/buoydata/past'\n",
    "\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the list of CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename the lat and lon columns to Latitude and Longitude\n",
    "combined_df.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Pad Month, Day, Hour, Min, and Sec columns with leading zeros\n",
    "combined_df['Month'] = combined_df['Month'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Day'] = combined_df['Day'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Hour'] = combined_df['Hour'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Min'] = combined_df['Min'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Sec'] = combined_df['Sec'].apply(lambda x: f'{x:02}')\n",
    "\n",
    "# Create a new column called datetime by combining Year, Month, Day, Hour, Min, and Sec columns\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['Year'].astype(str) + '-' +\n",
    "                                         combined_df['Month'].astype(str) + '-' +\n",
    "                                         combined_df['Day'].astype(str) + ' ' +\n",
    "                                         combined_df['Hour'].astype(str) + ':' +\n",
    "                                         combined_df['Min'].astype(str) + ':' +\n",
    "                                         combined_df['Sec'].astype(str))\n",
    "\n",
    "# Add a new column with the Day of Year (DOY) as an integer\n",
    "combined_df['DOY'] = combined_df['datetime'].dt.dayofyear\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaned buoy data geospatial bounds confirmation\n",
    "This cell will analyze and display the minimum and maximum values of the latitude and longitude fields of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the latitude and longitude ranges\n",
    "\n",
    "min_latitude = combined_df['Latitude'].min()\n",
    "max_latitude = combined_df['Latitude'].max()\n",
    "min_longitude = combined_df['Longitude'].min()\n",
    "max_longitude = combined_df['Longitude'].max()\n",
    "\n",
    "print(f\"Latitude: min = {min_latitude}, max = {max_latitude}\")\n",
    "print(f\"Longitude: min = {min_longitude}, max = {max_longitude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interpolate ERA5 reanalysis data to past buoy locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load the NetCDF files\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "# Extract the valid_time, latitudes, longitudes, and u-component wind values from the NetCDF file\n",
    "valid_time = uwnd_ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "latitudes = uwnd_ds.variables['latitude'][:]\n",
    "longitudes = uwnd_ds.variables['longitude'][:]\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Add a column to the dataframe called \"timestamp\"\n",
    "combined_df['timestamp'] = combined_df['datetime'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "\n",
    "# Create a KDTree for fast spatial lookup\n",
    "lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "# Add new columns to combined_df for the u-component and v-component wind values\n",
    "combined_df['era5_uwnd'] = np.nan\n",
    "combined_df['era5_vwnd'] = np.nan\n",
    "\n",
    "# Check the shape of the uwnd_array\n",
    "print(f\"uwnd_array shape: {uwnd_array.shape}\")\n",
    "print(f\"vwnd_array shape: {vwnd_array.shape}\")\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Find the value of the netCDF variable valid_time closest to the timestamp value\n",
    "    timestamp = row['timestamp']\n",
    "    time_diffs = np.abs(valid_time - timestamp)\n",
    "    closest_time_index = np.argmin(time_diffs)\n",
    "    \n",
    "    # Check if the calculated index is within the bounds of the uwnd_array\n",
    "    if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "        print(f\"Skipping row {index} with timestamp {timestamp} as it is out of bounds\")\n",
    "        continue\n",
    "    \n",
    "    # Select the corresponding netCDF slices\n",
    "    uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "    vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "    \n",
    "    # Find the grid cell of the netCDF slice closest to the Latitude and Longitude position\n",
    "    lat_lon = (row['Latitude'], row['Longitude'])\n",
    "    _, closest_point_index = tree.query(lat_lon)\n",
    "    closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "    \n",
    "    # Find the index of the closest latitude/longitude pair in the arrays\n",
    "    lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "    lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "    \n",
    "    # Assign the corresponding u and v values to the new columns in the dataframe\n",
    "    combined_df.at[index, 'era5_uwnd'] = uwnd_slice[lat_index, lon_index]\n",
    "    combined_df.at[index, 'era5_vwnd'] = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "# Drop the timestamp column from the dataframe\n",
    "combined_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Print the dataframe head\n",
    "print(combined_df.head())\n",
    "\n",
    "# Print a message saying the script has completed\n",
    "print(\"The script has completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined_df DataFrame to a CSV file\n",
    "output_path = '../data/cleaned/buoydata/past/IABP_2023_era5_interpolated.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
