{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing past buoy data and reanalyses for use in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all past buoy data into a single dataframe\n",
    "\n",
    "This section will collect all of the cleaned buoy data and combine them into a single dataframe. A column to represent the day of year (DOY) as an integer is also added. These data will be used (along with weather reanalyses) as training data for the machine learning model. Also removes buoys deployed outside of the arctic (<64 degrees N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multiple raw buoy CSV files into a single DataFrame and add a new column with the Day of Year (DOY) as an integer\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = '../data/cleaned/buoydata/past'\n",
    "\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the list of CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename the lat and lon columns to Latitude and Longitude\n",
    "combined_df.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Pad Month, Day, Hour, Min, and Sec columns with leading zeros\n",
    "combined_df['Month'] = combined_df['Month'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Day'] = combined_df['Day'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Hour'] = combined_df['Hour'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Min'] = combined_df['Min'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Sec'] = combined_df['Sec'].apply(lambda x: f'{x:02}')\n",
    "\n",
    "# Create a new column called datetime by combining Year, Month, Day, Hour, Min, and Sec columns\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['Year'].astype(str) + '-' +\n",
    "                                         combined_df['Month'].astype(str) + '-' +\n",
    "                                         combined_df['Day'].astype(str) + ' ' +\n",
    "                                         combined_df['Hour'].astype(str) + ':' +\n",
    "                                         combined_df['Min'].astype(str) + ':' +\n",
    "                                         combined_df['Sec'].astype(str))\n",
    "\n",
    "# Add a new column with the Day of Year (DOY) as an integer\n",
    "combined_df['DOY'] = combined_df['datetime'].dt.dayofyear\n",
    "\n",
    "# Iterate through the combined_df by BuoyID\n",
    "for buoy_id, group in combined_df.groupby('BuoyID'):\n",
    "    # Sort the records for each BuoyID by datetime from oldest to newest\n",
    "    group = group.sort_values(by='datetime')\n",
    "    \n",
    "    # Check if the first row of the sorted data has a latitude value less than 64\n",
    "    if group.iloc[0]['Latitude'] < 64:\n",
    "        # Remove the entire BuoyID from the dataset\n",
    "        combined_df = combined_df[combined_df['BuoyID'] != buoy_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate ERA5 to buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load the NetCDF files\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "# Extract the valid_time, latitudes, longitudes, and u-component wind values from the NetCDF file\n",
    "valid_time = uwnd_ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "latitudes = uwnd_ds.variables['latitude'][:]\n",
    "longitudes = uwnd_ds.variables['longitude'][:]\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Add a column to the dataframe called \"timestamp\"\n",
    "combined_df['timestamp'] = combined_df['datetime'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "\n",
    "# Create a KDTree for fast spatial lookup\n",
    "lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "# Add new columns to combined_df for the u-component and v-component wind values\n",
    "combined_df['era5_uwnd'] = np.nan\n",
    "combined_df['era5_vwnd'] = np.nan\n",
    "\n",
    "# Check the shape of the uwnd_array\n",
    "print(f\"uwnd_array shape: {uwnd_array.shape}\")\n",
    "print(f\"vwnd_array shape: {vwnd_array.shape}\")\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Find the value of the netCDF variable valid_time closest to the timestamp value\n",
    "    timestamp = row['timestamp']\n",
    "    time_diffs = np.abs(valid_time - timestamp)\n",
    "    closest_time_index = np.argmin(time_diffs)\n",
    "    \n",
    "    # Check if the calculated index is within the bounds of the uwnd_array\n",
    "    if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "        print(f\"Skipping row {index} with timestamp {timestamp} as it is out of bounds\")\n",
    "        continue\n",
    "    \n",
    "    # Select the corresponding netCDF slices\n",
    "    uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "    vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "    \n",
    "    # Find the grid cell of the netCDF slice closest to the Latitude and Longitude position\n",
    "    lat_lon = (row['Latitude'], row['Longitude'])\n",
    "    _, closest_point_index = tree.query(lat_lon)\n",
    "    closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "    \n",
    "    # Find the index of the closest latitude/longitude pair in the arrays\n",
    "    lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "    lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "    \n",
    "    # Assign the corresponding u and v values to the new columns in the dataframe\n",
    "    combined_df.at[index, 'era5_uwnd'] = uwnd_slice[lat_index, lon_index]\n",
    "    combined_df.at[index, 'era5_vwnd'] = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "# Drop the timestamp column from the dataframe\n",
    "combined_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Print the dataframe head\n",
    "print(combined_df.head())\n",
    "\n",
    "# Print a message saying the script has completed\n",
    "print(\"The ERA5 wind assignment script has completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate IBCAO v5 bathymetry to buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Save the combined DataFrame to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_buoy_data.csv', index=False)\n",
    "print(\"combined_df has been saved to 'combined_buoy_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more data to the spreadsheet (wind vector and displacement/heading columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading buoy data from the spreadsheet...\n",
      "Buoy data loaded successfully.\n",
      "            BuoyID  Year  Month  Day  Hour  Min  Sec  Latitude  Longitude  \\\n",
      "0  300025010734900  2023      8    7     0    7   32  77.33740 -138.15785   \n",
      "1  300025010734900  2023      8    7     0   51    5  77.33538 -138.13705   \n",
      "2  300025010734900  2023      8    7     1    1   40  77.33479 -138.13317   \n",
      "3  300025010734900  2023      8    7     2    1   21  77.33148 -138.11950   \n",
      "4  300025010734900  2023      8    7     3    1   11  77.32867 -138.12018   \n",
      "\n",
      "   GPSdelay    BPT      BP     Ts     Ta     Th  Batt             datetime  \\\n",
      "0         0 -999.0  1016.5 -999.0 -999.0  12.96    13  2023-08-07 00:07:32   \n",
      "1         0 -999.0  1016.5 -999.0 -999.0  13.86    13  2023-08-07 00:51:05   \n",
      "2         0 -999.0  1016.6 -999.0 -999.0  13.90    13  2023-08-07 01:01:40   \n",
      "3         0 -999.0  -999.0 -999.0 -999.0  12.74    13  2023-08-07 02:01:21   \n",
      "4         0 -999.0  1017.4 -999.0 -999.0  11.92    13  2023-08-07 03:01:11   \n",
      "\n",
      "   DOY  era5_uwnd  era5_vwnd  \n",
      "0  219  -6.943024   4.360657  \n",
      "1  219  -7.594177   4.592392  \n",
      "2  219  -7.594177   4.592392  \n",
      "3  219  -6.301437   3.599380  \n",
      "4  219  -3.661819   1.544037  \n",
      "Extracting necessary columns...\n",
      "Columns extracted successfully.\n",
      "   Latitude  Longitude           BuoyID             datetime  era5_uwnd  \\\n",
      "0  77.33740 -138.15785  300025010734900  2023-08-07 00:07:32  -6.943024   \n",
      "1  77.33538 -138.13705  300025010734900  2023-08-07 00:51:05  -7.594177   \n",
      "2  77.33479 -138.13317  300025010734900  2023-08-07 01:01:40  -7.594177   \n",
      "3  77.33148 -138.11950  300025010734900  2023-08-07 02:01:21  -6.301437   \n",
      "4  77.32867 -138.12018  300025010734900  2023-08-07 03:01:11  -3.661819   \n",
      "\n",
      "   era5_vwnd  \n",
      "0   4.360657  \n",
      "1   4.592392  \n",
      "2   4.592392  \n",
      "3   3.599380  \n",
      "4   1.544037  \n",
      "Rounding wind columns to two decimal places...\n",
      "Wind columns rounded successfully.\n",
      "Calculating wind magnitude and wind angle...\n",
      "Wind magnitude and wind angle calculated successfully.\n",
      "   Latitude  Longitude           BuoyID             datetime  era5_uwnd  \\\n",
      "0  77.33740 -138.15785  300025010734900  2023-08-07 00:07:32      -6.94   \n",
      "1  77.33538 -138.13705  300025010734900  2023-08-07 00:51:05      -7.59   \n",
      "2  77.33479 -138.13317  300025010734900  2023-08-07 01:01:40      -7.59   \n",
      "3  77.33148 -138.11950  300025010734900  2023-08-07 02:01:21      -6.30   \n",
      "4  77.32867 -138.12018  300025010734900  2023-08-07 03:01:11      -3.66   \n",
      "\n",
      "   era5_vwnd  wind_magnitude  wind_angle  \n",
      "0       4.36        8.195926  147.861233  \n",
      "1       4.59        8.869961  148.836839  \n",
      "2       4.59        8.869961  148.836839  \n",
      "3       3.60        7.256032  150.255119  \n",
      "4       1.54        3.970793  157.180344  \n",
      "Displaying the first few rows of the preprocessed data:\n",
      "Calculating displacement and heading...\n",
      "Displacement and heading calculated successfully.\n",
      "   Latitude  Longitude  BuoyID             datetime  era5_uwnd  era5_vwnd  \\\n",
      "0  81.53036 -149.67551  900115  2023-01-01 00:00:46      49.76     -13.68   \n",
      "1  81.53165 -149.68448  900115  2023-01-01 00:30:47      47.28     -12.56   \n",
      "2  81.53296 -149.69345  900115  2023-01-01 01:01:17      47.28     -12.56   \n",
      "3  81.53421 -149.70296  900115  2023-01-01 01:31:03      47.16     -14.88   \n",
      "4  81.53523 -149.71284  900115  2023-01-01 02:00:47      47.16     -14.88   \n",
      "\n",
      "   wind_magnitude  wind_angle  displacement     heading  \n",
      "0       51.606201  -15.371969      0.000000    0.000000  \n",
      "1       48.919853  -14.877097    205.312977  314.323130  \n",
      "2       48.919853  -14.877097    206.856929  314.768180  \n",
      "3       49.451795  -17.511613    208.707315  311.761871  \n",
      "4       49.451795  -17.511613    197.532749  305.046983  \n",
      "Processed buoy data saved to processed_buoy_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "from geopy import Point\n",
    "import math\n",
    "\n",
    "# Load and Preprocess Data\n",
    "\n",
    "print(\"Loading buoy data from the spreadsheet...\")\n",
    "\n",
    "# Load the buoy data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../combined_buoy_data.csv')\n",
    "print(\"Buoy data loaded successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "print(\"Extracting necessary columns...\")\n",
    "\n",
    "# Extract necessary columns\n",
    "buoy_data = buoy_data[['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd']]\n",
    "print(\"Columns extracted successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "print(\"Rounding wind columns to two decimal places...\")\n",
    "\n",
    "# Round wind columns to two decimal places\n",
    "buoy_data['era5_uwnd'] = buoy_data['era5_uwnd'].round(2)\n",
    "buoy_data['era5_vwnd'] = buoy_data['era5_vwnd'].round(2)\n",
    "print(\"Wind columns rounded successfully.\")\n",
    "\n",
    "print(\"Calculating wind magnitude and wind angle...\")\n",
    "\n",
    "# Calculate wind magnitude and wind angle\n",
    "buoy_data['wind_magnitude'] = np.sqrt(buoy_data['era5_uwnd']**2 + buoy_data['era5_vwnd']**2)\n",
    "buoy_data['wind_angle'] = np.degrees(np.arctan2(buoy_data['era5_vwnd'], buoy_data['era5_uwnd']))\n",
    "\n",
    "print(\"Wind magnitude and wind angle calculated successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "print(\"Displaying the first few rows of the preprocessed data:\")\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "buoy_data.head()\n",
    "\n",
    "print(\"Calculating displacement and heading...\")\n",
    "\n",
    "# Initialize displacement and heading columns\n",
    "buoy_data['displacement'] = 0.0\n",
    "buoy_data['heading'] = 0.0\n",
    "\n",
    "# Function to calculate displacement and heading for each group\n",
    "def calculate_displacement_and_heading(group):\n",
    "    group = group.sort_values(by='datetime').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        prev_point = Point(group.loc[i-1, 'Latitude'], group.loc[i-1, 'Longitude'])\n",
    "        curr_point = Point(group.loc[i, 'Latitude'], group.loc[i, 'Longitude'])\n",
    "        \n",
    "        # Calculate displacement\n",
    "        group.loc[i, 'displacement'] = great_circle(prev_point, curr_point).meters\n",
    "        \n",
    "        # Calculate heading\n",
    "        lat1, lon1 = math.radians(group.loc[i-1, 'Latitude']), math.radians(group.loc[i-1, 'Longitude'])\n",
    "        lat2, lon2 = math.radians(group.loc[i, 'Latitude']), math.radians(group.loc[i, 'Longitude'])\n",
    "        \n",
    "        dlon = lon2 - lon1\n",
    "        x = math.sin(dlon) * math.cos(lat2)\n",
    "        y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(dlon))\n",
    "        initial_heading = math.atan2(x, y)\n",
    "        initial_heading = math.degrees(initial_heading)\n",
    "        compass_heading = (initial_heading + 360) % 360\n",
    "        \n",
    "        group.loc[i, 'heading'] = compass_heading\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "buoy_data = buoy_data.groupby('BuoyID').apply(calculate_displacement_and_heading).reset_index(drop=True)\n",
    "\n",
    "print(\"Displacement and heading calculated successfully.\")\n",
    "print(buoy_data.head())\n",
    "\n",
    "# Save the processed buoy_data back to the spreadsheet\n",
    "output_csv_path = 'processed_buoy_data.csv'\n",
    "buoy_data.to_csv(output_csv_path, index=False)\n",
    "print(f\"Processed buoy data saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned buoy data geospatial bounds confirmation\n",
    "\n",
    "This cell will analyze and display the minimum and maximum values of the latitude and longitude fields of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the latitude and longitude ranges\n",
    "\n",
    "min_latitude = combined_df['Latitude'].min()\n",
    "max_latitude = combined_df['Latitude'].max()\n",
    "min_longitude = combined_df['Longitude'].min()\n",
    "max_longitude = combined_df['Longitude'].max()\n",
    "\n",
    "print(f\"Latitude: min = {min_latitude}, max = {max_latitude}\")\n",
    "print(f\"Longitude: min = {min_longitude}, max = {max_longitude}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
