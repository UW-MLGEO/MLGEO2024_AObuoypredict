{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import gc\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Handling\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Geospatial Calculations\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from haversine import haversine\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    VotingRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    Lasso,\n",
    "    LinearRegression,\n",
    "    Ridge\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Model Evaluation and Optimization\n",
    "import optuna\n",
    "from optuna import create_study\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    KFold,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing KDTree and time differences...\n",
      "KDTree and time differences precomputed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Coordinates: Latitude = 83.03172364000761, Longitude = -119.73791635149415\n"
     ]
    }
   ],
   "source": [
    "# Example: Predict one step using the function\n",
    "initial_coords = (83.0, -120.0)  # Starting latitude and longitude\n",
    "displacement_km = 5.0  # Displacement in kilometers\n",
    "heading_degrees = 45.0  # Heading in degrees from north\n",
    "\n",
    "# Calculate new position\n",
    "new_lat, new_lon = calculate_new_position(initial_coords, displacement_km, heading_degrees)\n",
    "print(f\"New Coordinates: Latitude = {new_lat}, Longitude = {new_lon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the math module\n",
    "import math\n",
    "\n",
    "# Redefine the calculate_new_position function with wrapping logic\n",
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    # Convert inputs to radians\n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    # Compute new latitude\n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    # Compute new longitude\n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    # Convert back to degrees\n",
    "    new_lat = math.degrees(lat2)\n",
    "    new_lon = math.degrees(lon2)\n",
    "    \n",
    "    # Wrap longitude to [-180, 180]\n",
    "    if new_lon > 180:\n",
    "        new_lon -= 360\n",
    "    elif new_lon < -180:\n",
    "        new_lon += 360\n",
    "    \n",
    "    return new_lat, new_lon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Evaluation Functions\n",
    "\n",
    "These functions handle model optimization with AutoML and evaluation of predictions. (implementation ongoing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(true_data, predicted_file):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of predictions against true data.\n",
    "    \"\"\"\n",
    "    # Read predicted data\n",
    "    pred_data = pd.read_csv(predicted_file)\n",
    "    pred_data['Datetime'] = pd.to_datetime(pred_data['Datetime'])\n",
    "    \n",
    "    # Merge true and predicted data on datetime\n",
    "    merged_data = pd.merge(\n",
    "        true_data,\n",
    "        pred_data,\n",
    "        left_on=['datetime'],\n",
    "        right_on=['Datetime'],\n",
    "        suffixes=('_true', '_pred')\n",
    "    )\n",
    "    \n",
    "    # Calculate position errors\n",
    "    position_errors = []\n",
    "    for _, row in merged_data.iterrows():\n",
    "        true_pos = (row['Latitude_true'], row['Longitude_true'])\n",
    "        pred_pos = (row['Latitude'], row['Longitude'])\n",
    "        error_km = haversine(true_pos, pred_pos)\n",
    "        position_errors.append(error_km)\n",
    "    \n",
    "    merged_data['position_error_km'] = position_errors\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'mean_position_error_km': np.mean(position_errors),\n",
    "        'median_position_error_km': np.median(position_errors),\n",
    "        'max_position_error_km': np.max(position_errors),\n",
    "        'std_position_error_km': np.std(position_errors),\n",
    "        'rmse_lat': np.sqrt(mean_squared_error(merged_data['Latitude_true'], merged_data['Latitude'])),\n",
    "        'rmse_lon': np.sqrt(mean_squared_error(merged_data['Longitude_true'], merged_data['Longitude'])),\n",
    "        'mae_lat': mean_absolute_error(merged_data['Latitude_true'], merged_data['Latitude']),\n",
    "        'mae_lon': mean_absolute_error(merged_data['Longitude_true'], merged_data['Longitude'])\n",
    "    }\n",
    "    \n",
    "    return metrics, merged_data\n",
    "\n",
    "def plot_trajectory_comparison(merged_data, buoy_id, model_name):\n",
    "    \"\"\"\n",
    "    Plot true vs predicted trajectories\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(merged_data['Longitude_true'], merged_data['Latitude_true'], \n",
    "             'b-', label='True Trajectory')\n",
    "    plt.plot(merged_data['Longitude'], merged_data['Latitude'], \n",
    "             'r--', label='Predicted Trajectory')\n",
    "    plt.title(f'True vs Predicted Trajectory - Buoy {buoy_id}\\nModel: {model_name}')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(f'../data/processed/predictions/trajectory_comparison_{buoy_id}_{model_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_all_predictions(val_data, predictions_dir):\n",
    "    \"\"\"\n",
    "    Evaluate all prediction files in the specified directory\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    prediction_files = glob.glob(f\"{predictions_dir}/predicted_*.csv\")\n",
    "    \n",
    "    for pred_file in prediction_files:\n",
    "        # Extract buoy_id and model_name from filename\n",
    "        filename = pred_file.split('/')[-1]\n",
    "        buoy_id = filename.split('_')[1]\n",
    "        model_name = filename.split('_')[2].replace('.csv', '')\n",
    "        \n",
    "        # Get true data for this buoy\n",
    "        true_data_buoy = val_data[val_data['BuoyID'] == int(buoy_id)]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics, merged_data = evaluate_predictions(true_data_buoy, pred_file)\n",
    "        metrics['buoy_id'] = buoy_id\n",
    "        metrics['model_name'] = model_name\n",
    "        \n",
    "        # Plot trajectory comparison\n",
    "        plot_trajectory_comparison(merged_data, buoy_id, model_name)\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f'{predictions_dir}/evaluation_results.csv', index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs):\n",
    "\n",
    "    # Initialize an empty list to store predictions for all buoys\n",
    "    all_predictions = []\n",
    "\n",
    "    # Iterate over each unique BuoyID\n",
    "    unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "    for buoy_id in unique_buoy_ids:\n",
    "        buoy_data = val_data[val_data['BuoyID'] == buoy_id]\n",
    "\n",
    "        # Initialize an empty list to store predictions for the current buoy\n",
    "        predictions = []\n",
    "\n",
    "        # Extract initial conditions for the current buoy\n",
    "        current_lat, current_lon = buoy_data.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = buoy_data.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "\n",
    "        # Add the initial condition as the first prediction\n",
    "        predictions.append([current_lat, current_lon, buoy_data.iloc[0]['datetime']])\n",
    "\n",
    "        for i in range(1, len(buoy_data)):\n",
    "            next_row = buoy_data.iloc[i]\n",
    "\n",
    "            # Prepare input data\n",
    "            input_data = pd.DataFrame({\n",
    "                'Latitude': [current_lat],\n",
    "                'Longitude': [current_lon],\n",
    "                'era5_uwnd': [current_uwnd],\n",
    "                'era5_vwnd': [current_vwnd]\n",
    "            })\n",
    "\n",
    "            # Make prediction for displacement and heading\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "\n",
    "            # Extract wind components at the predicted position and time\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "\n",
    "            # Append the prediction for the current buoy\n",
    "            predictions.append([predicted_lat, predicted_lon, next_row['datetime']])\n",
    "\n",
    "            # Update current state for the next iteration\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "        # Append predictions of the current buoy to all_predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Convert all predictions to a NumPy array before returning\n",
    "    all_predictions_array = np.array(all_predictions, dtype=object)\n",
    "    return all_predictions_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection, training, and validation (includes computation timing calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model: ElasticNet\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 827.064\n",
      "Fold 1 time: 380.83 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 767.791\n",
      "Fold 2 time: 408.17 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 825.619\n",
      "Fold 3 time: 413.79 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 813.975\n",
      "Fold 4 time: 409.12 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1829.535\n",
      "Fold 5 time: 414.53 seconds\n",
      "\n",
      "Completed cross-validation for ElasticNet. Mean RMSE: 1012.797, Std. Dev: 408.938, Total Time: 2026.44 seconds\n",
      "\n",
      "Testing model: GradientBoosting\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.468\n",
      "Fold 1 time: 984.29 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 768.219\n",
      "Fold 2 time: 949.54 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 821.637\n",
      "Fold 3 time: 927.72 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 818.564\n",
      "Fold 4 time: 928.62 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1830.465\n",
      "Fold 5 time: 930.42 seconds\n",
      "\n",
      "Completed cross-validation for GradientBoosting. Mean RMSE: 1012.671, Std. Dev: 409.423, Total Time: 4720.60 seconds\n",
      "\n",
      "Testing model: RandomForest\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.421\n",
      "Fold 1 time: 1031.53 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 766.104\n",
      "Fold 2 time: 1012.37 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 825.877\n",
      "Fold 3 time: 1013.98 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 822.726\n",
      "Fold 4 time: 1011.73 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1828.945\n",
      "Fold 5 time: 1010.71 seconds\n",
      "\n",
      "Completed cross-validation for RandomForest. Mean RMSE: 1013.615, Std. Dev: 408.290, Total Time: 5080.32 seconds\n",
      "\n",
      "Testing model: XGBoost\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.237\n",
      "Fold 1 time: 1087.02 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 766.923\n",
      "Fold 2 time: 1057.80 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 826.941\n",
      "Fold 3 time: 1061.85 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 812.149\n",
      "Fold 4 time: 1090.89 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1830.533\n",
      "Fold 5 time: 1145.53 seconds\n",
      "\n",
      "Completed cross-validation for XGBoost. Mean RMSE: 1012.157, Std. Dev: 409.756, Total Time: 5443.08 seconds\n",
      "\n",
      "Testing model: LightGBM\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.533\n",
      "Fold 1 time: 787.57 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 772.126\n",
      "Fold 2 time: 808.36 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 829.211\n",
      "Fold 3 time: 676.46 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 816.268\n",
      "Fold 4 time: 666.25 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1830.936\n",
      "Fold 5 time: 667.43 seconds\n",
      "\n",
      "Completed cross-validation for LightGBM. Mean RMSE: 1014.615, Std. Dev: 408.663, Total Time: 3606.07 seconds\n",
      "\n",
      "=== Best model selected: XGBoost ===\n",
      "Mean RMSE: 1012.157, Total Time: 5443.08 seconds\n",
      "Best model: XGBoost\n",
      "Mean RMSE: 1012.157\n",
      "Total Time: 5443.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# LightGBM verbosity suppression\n",
    "lgb_params = {'verbose': -1}\n",
    "\n",
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../combined_buoy_data.csv')\n",
    "\n",
    "# Drop unused columns\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "\n",
    "# Define features and targets\n",
    "X = buoy_data[['Latitude', 'Longitude', 'era5_uwnd', 'era5_vwnd', 'BuoyID', 'datetime']]\n",
    "y = buoy_data[['displacement', 'heading']]\n",
    "groups = buoy_data['BuoyID']\n",
    "\n",
    "# Models to evaluate\n",
    "model_configs = [\n",
    "    ('ElasticNet', MultiOutputRegressor(ElasticNet(alpha=1.0, l1_ratio=0.5))),\n",
    "    ('GradientBoosting', MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=5))),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=100, max_depth=10)),\n",
    "    ('XGBoost', MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=6, objective='reg:squarederror'))),\n",
    "    ('LightGBM', MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, max_depth=6, **lgb_params)))\n",
    "]\n",
    "\n",
    "# GroupKFold for cross-validation\n",
    "cv_folds = 5\n",
    "group_kf = GroupKFold(n_splits=cv_folds)\n",
    "\n",
    "# Ensure the predictions directory exists\n",
    "predictions_dir = '../data/processed/predictions'\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Cross-validation\n",
    "for model_name, model in model_configs:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    model_scores = []  # To store RMSE for each fold\n",
    "    fold_times = []  # To store time taken for each fold\n",
    "\n",
    "    for fold_num, (train_index, val_index) in enumerate(group_kf.split(X, y, groups=groups)):\n",
    "        print(f\"\\nFold {fold_num + 1}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Retain 'BuoyID' in X_val for iteration step\n",
    "        X_val_with_buoyid = X_val.copy()\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime'])\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict iteratively\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X_val_with_buoyid,\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Convert predictions to a DataFrame for easier handling\n",
    "        y_pred = pd.DataFrame(y_pred, columns=['Latitude', 'Longitude', 'datetime'])\n",
    "\n",
    "        # Exclude the datetime column for RMSE calculation and ensure numeric dtype\n",
    "        y_pred_numeric = np.array(y_pred[['Latitude', 'Longitude']].to_numpy(), dtype=np.float64)\n",
    "\n",
    "        # Ensure y_val is in the same format\n",
    "        y_val_numeric = y_val.to_numpy()\n",
    "\n",
    "        # Calculate RMSE\n",
    "        try:\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_numeric, y_pred_numeric))\n",
    "            model_scores.append(rmse)\n",
    "            print(f\"Fold {fold_num + 1} RMSE: {rmse:.3f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error calculating RMSE: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Record time taken for the fold\n",
    "        fold_time = time.time() - start_time\n",
    "        fold_times.append(fold_time)\n",
    "        print(f\"Fold {fold_num + 1} time: {fold_time:.2f} seconds\")\n",
    "\n",
    "        # Save predictions and true values to CSV\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'BuoyID': X_val_with_buoyid['BuoyID'].values,  # Add BuoyID to the output\n",
    "            'True Latitude': X_val_with_buoyid['Latitude'].values,  # Use latitude from X_val_with_buoyid\n",
    "            'True Longitude': X_val_with_buoyid['Longitude'].values,  # Use longitude from X_val_with_buoyid\n",
    "            'Predicted Latitude': np.round(y_pred_numeric[:, 0], 4),  # Predicted latitude rounded to 4 decimal places\n",
    "            'Predicted Longitude': np.round(y_pred_numeric[:, 1], 4)  # Predicted longitude rounded to 4 decimal places\n",
    "        })\n",
    "        predictions_file = os.path.join(predictions_dir, f\"{model_name}_fold{fold_num + 1}_predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "    # Store results for this model\n",
    "    mean_rmse = np.mean(model_scores)\n",
    "    std_rmse = np.std(model_scores)\n",
    "    total_time = sum(fold_times)\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Mean RMSE': mean_rmse,\n",
    "        'RMSE StdDev': std_rmse,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Mean Time per Fold (s)': np.mean(fold_times)\n",
    "    })\n",
    "\n",
    "    print(f\"\\nCompleted cross-validation for {model_name}. \"\n",
    "          f\"Mean RMSE: {mean_rmse:.3f}, Std. Dev: {std_rmse:.3f}, Total Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "\n",
    "# Identify the best model based on mean RMSE\n",
    "best_model_row = results_df.loc[results_df['Mean RMSE'].idxmin()]\n",
    "print(f\"\\n=== Best model selected: {best_model_row['Model']} ===\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']:.3f}, Total Time: {best_model_row['Total Time (s)']:.2f} seconds\")\n",
    "\n",
    "# Store the best model\n",
    "best_model = model_configs[results_df['Mean RMSE'].idxmin()][1]\n",
    "\n",
    "print(f\"Best model: {best_model_row['Model']}\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']:.3f}\")\n",
    "print(f\"Total Time: {best_model_row['Total Time (s)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning on the best model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 09:29:31,329] A new study created in memory with name: no-name-3b7c8661-3f3a-41d7-9542-5d88cb88f0b4\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import optuna\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    if best_model_row['Model'] == 'ElasticNet':\n",
    "        alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "        l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "        model = MultiOutputRegressor(ElasticNet(alpha=alpha, l1_ratio=l1_ratio))\n",
    "    elif best_model_row['Model'] == 'GradientBoosting':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth))\n",
    "    elif best_model_row['Model'] == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 5, 15)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    elif best_model_row['Model'] == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        model = MultiOutputRegressor(XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, objective='reg:squarederror'))\n",
    "    elif best_model_row['Model'] == 'LightGBM':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        model = MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate))\n",
    "\n",
    "    # Function to process one fold (to parallelize)\n",
    "    def process_fold(train_index, val_index):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Convert to numpy arrays and reduce precision\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime']).to_numpy(dtype='float32')\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime']).to_numpy(dtype='float32')\n",
    "        y_train = y_train.to_numpy(dtype='float32')\n",
    "        y_val = y_val.to_numpy(dtype='float32')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Iterative prediction\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X.iloc[val_index],\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Filter out datetime column from predictions\n",
    "        y_pred_filtered = y_pred[:, :2]  # Keep only Longitude and Latitude\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_filtered))\n",
    "\n",
    "        # Free memory\n",
    "        del X_train, X_val, y_train, y_val, y_pred, y_pred_filtered\n",
    "        gc.collect()\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    # Limit number of folds to 3\n",
    "    folds = list(group_kf.split(X, y, groups=groups))[:3]\n",
    "\n",
    "    # Parallel cross-validation using joblib\n",
    "    model_scores = Parallel(n_jobs=-1)(delayed(process_fold)(train_idx, val_idx) for train_idx, val_idx in folds)\n",
    "\n",
    "    # Pruning - Stop unpromising trials early\n",
    "    intermediate_value = np.mean(model_scores)\n",
    "    trial.report(intermediate_value, step=0)  # Only one step per trial here\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return intermediate_value\n",
    "\n",
    "\n",
    "# Create an Optuna study with pruning\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    ")\n",
    "\n",
    "# Optimize with reduced trials\n",
    "study.optimize(objective, n_trials=20)  # Reduced to 20 trials\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with the best tuned model and saving the results for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data while ensuring group integrity\n",
    "train_idx, eval_idx = next(GroupKFold(n_splits=5).split(X, y, groups=groups))\n",
    "X_train, X_eval = X.iloc[train_idx], X.iloc[eval_idx]\n",
    "y_train, y_eval = y.iloc[train_idx], y.iloc[eval_idx]\n",
    "\n",
    "# Drop 'BuoyID' and 'datetime' for training the model\n",
    "X_train_clean = X_train.drop(columns=['BuoyID', 'datetime'])\n",
    "X_eval_clean = X_eval.drop(columns=['BuoyID', 'datetime'])\n",
    "\n",
    "# Instantiate the model using best_params\n",
    "if best_model_row['Model'] == 'ElasticNet':\n",
    "    best_model = MultiOutputRegressor(ElasticNet(**best_params))\n",
    "elif best_model_row['Model'] == 'GradientBoosting':\n",
    "    best_model = MultiOutputRegressor(GradientBoostingRegressor(**best_params))\n",
    "elif best_model_row['Model'] == 'RandomForest':\n",
    "    best_model = RandomForestRegressor(**best_params)\n",
    "elif best_model_row['Model'] == 'XGBoost':\n",
    "    best_model = MultiOutputRegressor(XGBRegressor(**best_params, objective='reg:squarederror'))\n",
    "elif best_model_row['Model'] == 'LightGBM':\n",
    "    best_model = MultiOutputRegressor(lgb.LGBMRegressor(**best_params))\n",
    "\n",
    "# Fit the tuned model on the training data\n",
    "best_model.fit(X_train_clean, y_train)\n",
    "\n",
    "# Prepare for iterative predictions\n",
    "X_eval_with_buoyid = X_eval.copy()\n",
    "\n",
    "# Use the iterative_prediction function for evaluation\n",
    "y_pred = iterative_prediction(\n",
    "    val_data=X_eval_with_buoyid,\n",
    "    model=best_model,\n",
    "    tree=tree,\n",
    "    valid_times=valid_time_dt,\n",
    "    latitudes=latitudes,\n",
    "    longitudes=longitudes,\n",
    "    lat_lon_pairs=lat_lon_pairs\n",
    ")\n",
    "\n",
    "# Convert predictions to a DataFrame for comparison\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['Latitude', 'Longitude'])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "y_pred_numeric = y_pred_df.to_numpy()\n",
    "y_eval_numeric = y_eval.to_numpy()\n",
    "\n",
    "# RMSE\n",
    "eval_rmse = np.sqrt(mean_squared_error(y_eval_numeric, y_pred_numeric))\n",
    "print(f\"Evaluation RMSE: {eval_rmse:.3f}\")\n",
    "\n",
    "# Save predictions for analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'True Latitude': y_eval['Latitude'].values,\n",
    "    'True Longitude': y_eval['Longitude'].values,\n",
    "    'Predicted Latitude': y_pred_df['Latitude'].values,\n",
    "    'Predicted Longitude': y_pred_df['Longitude'].values\n",
    "})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_file = '../data/processed/predictions/tuned_model_predictions.csv'\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "print(f\"Predictions saved to: {predictions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all predictions\n",
    "print(\"Evaluating predictions...\")\n",
    "results_df = evaluate_all_predictions(val_data, '../data/processed/predictions/')\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nMean metrics across all buoys:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "print(\"\\nMetrics by model:\")\n",
    "print(results_df.groupby('model_name').mean(numeric_only=True))\n",
    "\n",
    "# Plot overall error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results_df['mean_position_error_km'], bins=20)\n",
    "plt.title('Distribution of Mean Position Errors')\n",
    "plt.xlabel('Mean Position Error (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('../data/processed/predictions/error_distribution.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlggeo2024_aobuoypredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
