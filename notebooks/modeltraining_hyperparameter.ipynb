{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso, BayesianRidge\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import optuna\n",
    "from scipy.stats import randint, uniform\n",
    "import glob\n",
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing KDTree and time differences...\n",
      "KDTree and time differences precomputed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    return math.degrees(lat2), math.degrees(lon2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Evaluation Functions\n",
    "\n",
    "These functions handle model optimization with AutoML and evaluation of predictions. (implementation ongoing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(true_data, predicted_file):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of predictions against true data.\n",
    "    \"\"\"\n",
    "    # Read predicted data\n",
    "    pred_data = pd.read_csv(predicted_file)\n",
    "    pred_data['Datetime'] = pd.to_datetime(pred_data['Datetime'])\n",
    "    \n",
    "    # Merge true and predicted data on datetime\n",
    "    merged_data = pd.merge(\n",
    "        true_data,\n",
    "        pred_data,\n",
    "        left_on=['datetime'],\n",
    "        right_on=['Datetime'],\n",
    "        suffixes=('_true', '_pred')\n",
    "    )\n",
    "    \n",
    "    # Calculate position errors\n",
    "    position_errors = []\n",
    "    for _, row in merged_data.iterrows():\n",
    "        true_pos = (row['Latitude_true'], row['Longitude_true'])\n",
    "        pred_pos = (row['Latitude'], row['Longitude'])\n",
    "        error_km = haversine(true_pos, pred_pos)\n",
    "        position_errors.append(error_km)\n",
    "    \n",
    "    merged_data['position_error_km'] = position_errors\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'mean_position_error_km': np.mean(position_errors),\n",
    "        'median_position_error_km': np.median(position_errors),\n",
    "        'max_position_error_km': np.max(position_errors),\n",
    "        'std_position_error_km': np.std(position_errors),\n",
    "        'rmse_lat': np.sqrt(mean_squared_error(merged_data['Latitude_true'], merged_data['Latitude'])),\n",
    "        'rmse_lon': np.sqrt(mean_squared_error(merged_data['Longitude_true'], merged_data['Longitude'])),\n",
    "        'mae_lat': mean_absolute_error(merged_data['Latitude_true'], merged_data['Latitude']),\n",
    "        'mae_lon': mean_absolute_error(merged_data['Longitude_true'], merged_data['Longitude'])\n",
    "    }\n",
    "    \n",
    "    return metrics, merged_data\n",
    "\n",
    "def plot_trajectory_comparison(merged_data, buoy_id, model_name):\n",
    "    \"\"\"\n",
    "    Plot true vs predicted trajectories\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(merged_data['Longitude_true'], merged_data['Latitude_true'], \n",
    "             'b-', label='True Trajectory')\n",
    "    plt.plot(merged_data['Longitude'], merged_data['Latitude'], \n",
    "             'r--', label='Predicted Trajectory')\n",
    "    plt.title(f'True vs Predicted Trajectory - Buoy {buoy_id}\\nModel: {model_name}')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(f'../data/processed/predictions/trajectory_comparison_{buoy_id}_{model_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "def objective(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 100),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "    \n",
    "    model = RandomForestRegressor(**params, n_jobs=-1)\n",
    "    scores = cross_val_score(\n",
    "        model, X_train, y_train, \n",
    "        cv=3, scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    return -np.mean(scores)\n",
    "\n",
    "def train_optimized_model(X_train, y_train, n_trials=100):\n",
    "    \"\"\"\n",
    "    Train model with optimized hyperparameters using Optuna\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train),\n",
    "        n_trials=n_trials\n",
    "    )\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    best_model = RandomForestRegressor(**best_params, n_jobs=-1)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "def evaluate_all_predictions(val_data, predictions_dir):\n",
    "    \"\"\"\n",
    "    Evaluate all prediction files in the specified directory\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    prediction_files = glob.glob(f\"{predictions_dir}/predicted_*.csv\")\n",
    "    \n",
    "    for pred_file in prediction_files:\n",
    "        # Extract buoy_id and model_name from filename\n",
    "        filename = pred_file.split('/')[-1]\n",
    "        buoy_id = filename.split('_')[1]\n",
    "        model_name = filename.split('_')[2].replace('.csv', '')\n",
    "        \n",
    "        # Get true data for this buoy\n",
    "        true_data_buoy = val_data[val_data['BuoyID'] == int(buoy_id)]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics, merged_data = evaluate_predictions(true_data_buoy, pred_file)\n",
    "        metrics['buoy_id'] = buoy_id\n",
    "        metrics['model_name'] = model_name\n",
    "        \n",
    "        # Plot trajectory comparison\n",
    "        plot_trajectory_comparison(merged_data, buoy_id, model_name)\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f'{predictions_dir}/evaluation_results.csv', index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs):\n",
    "    start_time = time.time()\n",
    "    max_duration = 4 * 60 * 60  # Maximum runtime in seconds\n",
    "\n",
    "    # Initialize an empty list to store predictions for all buoys\n",
    "    all_predictions = []\n",
    "\n",
    "    # Iterate over each unique BuoyID\n",
    "    unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "    for buoy_id in unique_buoy_ids:\n",
    "        print(f\"Processing BuoyID: {buoy_id}\")\n",
    "        buoy_data = val_data[val_data['BuoyID'] == buoy_id]\n",
    "\n",
    "        # Initialize an empty list to store predictions for the current buoy\n",
    "        predictions = []\n",
    "\n",
    "        # Extract initial conditions for the current buoy\n",
    "        current_lat, current_lon = buoy_data.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = buoy_data.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "\n",
    "        for i in range(1, len(buoy_data)):\n",
    "            # Check if the maximum duration has been exceeded\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > max_duration:\n",
    "                print(\"Maximum duration exceeded. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            next_row = buoy_data.iloc[i]\n",
    "\n",
    "            input_data = pd.DataFrame({\n",
    "                'Latitude': [current_lat],\n",
    "                'Longitude': [current_lon],\n",
    "                'era5_uwnd': [current_uwnd],\n",
    "                'era5_vwnd': [current_vwnd]\n",
    "            })\n",
    "\n",
    "            # Make prediction for displacement and heading\n",
    "            prediction_start_time = time.time()\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "            prediction_end_time = time.time()\n",
    "\n",
    "            # Extract wind components at the predicted position and time\n",
    "            wind_extraction_start_time = time.time()\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "            wind_extraction_end_time = time.time()\n",
    "\n",
    "            # Append the prediction for the current buoy\n",
    "            predictions.append([\n",
    "                predicted_lat,\n",
    "                predicted_lon,\n",
    "                next_row['datetime']\n",
    "            ])\n",
    "\n",
    "            # Update current state for the next iteration\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "        # Append predictions of the current buoy to all_predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Convert all predictions to a NumPy array before returning\n",
    "    all_predictions_array = np.array(all_predictions, dtype=object)\n",
    "    print(f\"\\nPrediction complete. Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "    return all_predictions_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection, training, and validation (includes computation timing calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unused columns and retaining only: ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
      "Datetime column successfully converted to datetime format.\n",
      "Testing model: ElasticNet\n",
      "Fold 1: 47 unique buoys in validation set\n",
      "Training model on fold 1...\n",
      "Model ElasticNet trained on Fold 1\n",
      "Processing BuoyID: 900124\n",
      "Processing BuoyID: 900130\n",
      "Processing BuoyID: 900138\n",
      "Processing BuoyID: 902005\n",
      "Processing BuoyID: 300025060513230\n",
      "Processing BuoyID: 300234060321940\n",
      "Processing BuoyID: 300234060330560\n",
      "Processing BuoyID: 300234060727760\n",
      "Processing BuoyID: 300234060728940\n",
      "Processing BuoyID: 300234061164500\n",
      "Processing BuoyID: 300234066088170\n",
      "Processing BuoyID: 300234066534020\n",
      "Processing BuoyID: 300234066691600\n",
      "Processing BuoyID: 300234066797650\n",
      "Processing BuoyID: 300234066891240\n",
      "Processing BuoyID: 300234066892270\n",
      "Processing BuoyID: 300234066893240\n",
      "Processing BuoyID: 300234066894280\n",
      "Processing BuoyID: 300234067977270\n",
      "Processing BuoyID: 300234068242250\n",
      "Processing BuoyID: 300234068487020\n",
      "Processing BuoyID: 300234068762710\n",
      "Processing BuoyID: 300234068763720\n",
      "Processing BuoyID: 300234068763750\n",
      "Processing BuoyID: 300234068764770\n",
      "Processing BuoyID: 300534061353000\n",
      "Processing BuoyID: 300534061512090\n",
      "Processing BuoyID: 300534061785890\n",
      "Processing BuoyID: 300534062020970\n",
      "Processing BuoyID: 300534062026670\n",
      "Processing BuoyID: 300534062123830\n",
      "Processing BuoyID: 300534062720770\n",
      "Processing BuoyID: 300534062722770\n",
      "Processing BuoyID: 300534062726500\n",
      "Processing BuoyID: 300534062893740\n",
      "Processing BuoyID: 300534063012410\n",
      "Processing BuoyID: 300534063012420\n",
      "Processing BuoyID: 300534063015430\n",
      "Processing BuoyID: 300534063054440\n",
      "Processing BuoyID: 300534063054450\n",
      "Processing BuoyID: 300534063056460\n",
      "Processing BuoyID: 300534063058450\n",
      "Processing BuoyID: 300534063058460\n",
      "Processing BuoyID: 300534063449630\n",
      "Processing BuoyID: 300534063486690\n",
      "Processing BuoyID: 300534063805140\n",
      "Processing BuoyID: 300534064264630\n",
      "\n",
      "Prediction complete. Total predictions: 319711\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [319758, 319711]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 83\u001b[0m\n\u001b[0;32m     72\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m iterative_prediction(\n\u001b[0;32m     73\u001b[0m     val_data\u001b[38;5;241m=\u001b[39mX_val_with_buoyid,  \u001b[38;5;66;03m# Pass full data with 'BuoyID' to iterator\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     lat_lon_pairs\u001b[38;5;241m=\u001b[39mlat_lon_pairs\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Calculate RMSE for this fold\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_val, y_pred))\n\u001b[0;32m     84\u001b[0m model_scores\u001b[38;5;241m.\u001b[39mappend(rmse)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    498\u001b[0m     y_true, y_pred, multioutput\n\u001b[0;32m    499\u001b[0m )\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m    103\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    104\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [319758, 319711]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # To track timing\n",
    "\n",
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../combined_buoy_data.csv')\n",
    "\n",
    "# Drop unused columns\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
    "print(\"Dropping unused columns and retaining only:\", columns_to_keep)\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "print(\"Datetime column successfully converted to datetime format.\")\n",
    "\n",
    "# Define features and targets\n",
    "X = buoy_data[['Latitude', 'Longitude', 'era5_uwnd', 'era5_vwnd', 'BuoyID', 'datetime']]\n",
    "y = buoy_data[['displacement', 'heading']]\n",
    "groups = buoy_data['BuoyID']\n",
    "\n",
    "# Models to evaluate with proper multi-output handling\n",
    "model_configs = [\n",
    "     ('ElasticNet', MultiOutputRegressor(ElasticNet(alpha=1.0, l1_ratio=0.5))),\n",
    "    ('GradientBoosting', MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=5))),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=100, max_depth=10)),\n",
    "    ('XGBoost', MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=6, objective='reg:squarederror'))),\n",
    "    ('LightGBM', MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, max_depth=6)))\n",
    "]\n",
    "\n",
    "# GroupKFold for cross-validation\n",
    "cv_folds = 5\n",
    "group_kf = GroupKFold(n_splits=cv_folds)\n",
    "\n",
    "# Dictionary to store cross-validation results and timing\n",
    "cv_results = {}\n",
    "timing_results = {}\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model in model_configs:\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    \n",
    "    model_scores = []\n",
    "    fold_times = []\n",
    "\n",
    "    # Perform GroupKFold cross-validation\n",
    "    for fold_num, (train_index, val_index) in enumerate(group_kf.split(X, y, groups=groups)):\n",
    "        fold_buoys = buoy_data.iloc[val_index]['BuoyID'].nunique()\n",
    "        print(f\"Fold {fold_num + 1}: {fold_buoys} unique buoys in validation set\")\n",
    "        \n",
    "        # Split the data into training and validation\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Retain 'BuoyID' in X_val for iteration step\n",
    "        X_val_with_buoyid = X_val.copy()\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime'])  # Exclude 'BuoyID' for training\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime'])  # Exclude 'BuoyID' for validation features\n",
    "\n",
    "        # Measure training and validation time\n",
    "        start_time = time.time()\n",
    "        print(f\"Training model on fold {fold_num+1}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"Model {model_name} trained on Fold {fold_num+1}\")\n",
    "        \n",
    "        # Make iterative predictions, using X_val_with_buoyid\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X_val_with_buoyid,  # Pass full data with 'BuoyID' to iterator\n",
    "            model=model, \n",
    "            tree=tree, \n",
    "            valid_times=valid_time_dt, \n",
    "            latitudes=latitudes, \n",
    "            longitudes=longitudes, \n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Calculate RMSE for this fold\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        model_scores.append(rmse)\n",
    "        print(f\"Fold {fold_num+1} RMSE: {rmse:.3f}\")\n",
    "\n",
    "        # Record the time taken for this fold\n",
    "        fold_time = time.time() - start_time\n",
    "        fold_times.append(fold_time)\n",
    "        print(f\"Fold {fold_num+1} time taken: {fold_time:.2f} seconds\")\n",
    "    \n",
    "    # Store cross-validation results\n",
    "    cv_results[model_name] = model_scores\n",
    "    timing_results[model_name] = fold_times\n",
    "    print(f\"\\nCross-validation completed for {model_name}. \"\n",
    "          f\"Mean RMSE: {np.mean(model_scores):.3f}, Std. Dev: {np.std(model_scores):.3f}\\n\"\n",
    "          f\"Mean time: {np.mean(fold_times):.2f} seconds, Total time: {np.sum(fold_times):.2f} seconds\\n\")\n",
    "\n",
    "# Select the best model based on mean RMSE\n",
    "best_model_name = min(cv_results, key=lambda k: np.mean(cv_results[k]))\n",
    "best_model = dict(model_configs)[best_model_name]\n",
    "print(f\"\\n=== Best model selected: {best_model_name} with mean RMSE: {np.mean(cv_results[best_model_name]):.3f} ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization with AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning on the best model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 20:35:09,247] A new study created in memory with name: no-name-d4c3eda4-6395-409f-85c0-8ffcf1f7d5bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training optimized model with Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 20:37:27,612] Trial 0 finished with value: 1016308.9119413403 and parameters: {'n_estimators': 404, 'max_depth': 28, 'min_samples_split': 15, 'min_samples_leaf': 8, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 1016308.9119413403.\n",
      "[I 2024-11-21 20:44:49,922] Trial 1 finished with value: 1126776.3119643775 and parameters: {'n_estimators': 914, 'max_depth': 63, 'min_samples_split': 15, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 0 with value: 1016308.9119413403.\n",
      "[I 2024-11-21 20:49:13,119] Trial 2 finished with value: 1187049.5309834296 and parameters: {'n_estimators': 520, 'max_depth': 43, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 0 with value: 1016308.9119413403.\n",
      "[I 2024-11-21 20:53:36,569] Trial 3 finished with value: 1188383.1698805906 and parameters: {'n_estimators': 625, 'max_depth': 18, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 0 with value: 1016308.9119413403.\n",
      "[W 2024-11-21 20:53:36,678] Trial 4 failed with parameters: {'n_estimators': 303, 'max_depth': 82, 'min_samples_split': 10, 'min_samples_leaf': 8, 'max_features': 'auto', 'bootstrap': False} because of the following error: ValueError('\\nAll the 3 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n3 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"c:\\\\Users\\\\benem\\\\anaconda3\\\\envs\\\\mlggeo2024_aobuoypredict\\\\Lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py\", line 888, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"c:\\\\Users\\\\benem\\\\anaconda3\\\\envs\\\\mlggeo2024_aobuoypredict\\\\Lib\\\\site-packages\\\\sklearn\\\\base.py\", line 1466, in wrapper\\n    estimator._validate_params()\\n  File \"c:\\\\Users\\\\benem\\\\anaconda3\\\\envs\\\\mlggeo2024_aobuoypredict\\\\Lib\\\\site-packages\\\\sklearn\\\\base.py\", line 666, in _validate_params\\n    validate_parameter_constraints(\\n  File \"c:\\\\Users\\\\benem\\\\anaconda3\\\\envs\\\\mlggeo2024_aobuoypredict\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_param_validation.py\", line 95, in validate_parameter_constraints\\n    raise InvalidParameterError(\\nsklearn.utils._param_validation.InvalidParameterError: The \\'max_features\\' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {\\'sqrt\\', \\'log2\\'} or None. Got \\'auto\\' instead.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\benem\\AppData\\Local\\Temp\\ipykernel_19904\\2496776517.py\", line 88, in <lambda>\n",
      "    lambda trial: objective(trial, X_train, y_train),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\benem\\AppData\\Local\\Temp\\ipykernel_19904\\2496776517.py\", line 75, in objective\n",
      "    scores = cross_val_score(\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 712, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 443, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 529, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 3 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "[W 2024-11-21 20:53:36,679] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train optimized model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining optimized model with Optuna...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m train_optimized_model(X_train, y_train, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest parameters found:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n",
      "Cell \u001b[1;32mIn[5], line 87\u001b[0m, in \u001b[0;36mtrain_optimized_model\u001b[1;34m(X_train, y_train, n_trials)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03mTrain model with optimized hyperparameters using Optuna\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, X_train, y_train),\n\u001b[0;32m     89\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mn_trials\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[5], line 88\u001b[0m, in \u001b[0;36mtrain_optimized_model.<locals>.<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03mTrain model with optimized hyperparameters using Optuna\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     87\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, X_train, y_train),\n\u001b[0;32m     89\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mn_trials\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n",
      "Cell \u001b[1;32mIn[5], line 75\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X_train, y_train)\u001b[0m\n\u001b[0;32m     65\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m])\n\u001b[0;32m     72\u001b[0m }\n\u001b[0;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[0;32m     76\u001b[0m     model, X_train, y_train, \n\u001b[0;32m     77\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(scores)\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    713\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    714\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    715\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    716\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    717\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    718\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    719\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    720\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    721\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    722\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    723\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    724\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    725\u001b[0m )\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[1;32m--> 443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32mc:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n    estimator._validate_params()\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n"
     ]
    }
   ],
   "source": [
    "# Train optimized model\n",
    "print(\"Training optimized model with Optuna...\")\n",
    "best_model, best_params = train_optimized_model(X_train, y_train, n_trials=100)\n",
    "print(\"\\nBest parameters found:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the entire dataset with tuned best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the best model on the entire training dataset\n",
    "print(\"Training best model...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Ensure the predictions directory exists\n",
    "predictions_dir = '../data/predictions'\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.makedirs(predictions_dir)\n",
    "    print(f\"Directory {predictions_dir} created.\")\n",
    "else:\n",
    "    print(f\"Directory {predictions_dir} already exists.\")\n",
    "\n",
    "# Get unique BuoyIDs\n",
    "unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "\n",
    "# Iterate over each BuoyID in the validation data and make predictions\n",
    "for buoy_id in unique_buoy_ids:\n",
    "    # Set output file path\n",
    "    output_file_path = f'../data/predictions/predictions_{buoy_id}.csv'\n",
    "    \n",
    "    # Subset the data for the current BuoyID\n",
    "    buoy_data = val_data[val_data['BuoyID'] == buoy_id]\n",
    "\n",
    "    # Run iterative predictions on validation subset\n",
    "    print(\"\\nStarting iterative predictions on validation subset...\")\n",
    "    iterative_prediction(\n",
    "        val_data=buoy_data,\n",
    "        model=best_model,\n",
    "        tree=tree,\n",
    "        valid_times=valid_time_dt,\n",
    "        latitudes=latitudes,\n",
    "        longitudes=longitudes,\n",
    "        lat_lon_pairs=lat_lon_pairs,\n",
    "        output_file_path=output_file_path\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all predictions\n",
    "print(\"Evaluating predictions...\")\n",
    "results_df = evaluate_all_predictions(val_data, '../data/processed/predictions/')\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"\\nMean metrics across all buoys:\")\n",
    "print(results_df.mean(numeric_only=True))\n",
    "print(\"\\nMetrics by model:\")\n",
    "print(results_df.groupby('model_name').mean(numeric_only=True))\n",
    "\n",
    "# Plot overall error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results_df['mean_position_error_km'], bins=20)\n",
    "plt.title('Distribution of Mean Position Errors')\n",
    "plt.xlabel('Mean Position Error (km)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('../data/processed/predictions/error_distribution.png')\n",
    "plt.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
