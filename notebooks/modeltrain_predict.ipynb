{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../processed_buoy_data.csv')\n",
    "\n",
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    return math.degrees(lat2), math.degrees(lon2)\n",
    "\n",
    "# Drop unused columns\n",
    "print(\"Dropping unused columns...\")\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "print(\"Datetime column converted.\")\n",
    "\n",
    "# Split data by BuoyID into training and validation sets\n",
    "print(\"Splitting data by BuoyID...\")\n",
    "buoy_ids = buoy_data['BuoyID'].unique()\n",
    "train_ids = np.random.choice(buoy_ids, size=int(len(buoy_ids) - 5), replace=False)\n",
    "val_ids = np.setdiff1d(buoy_ids, train_ids)\n",
    "\n",
    "train_data = buoy_data[buoy_data['BuoyID'].isin(train_ids)].copy()\n",
    "val_data = buoy_data[buoy_data['BuoyID'].isin(val_ids)].copy()\n",
    "\n",
    "# Clear unused data from memory\n",
    "del train_data, val_data\n",
    "gc.collect()\n",
    "print(\"Memory cleaned up after subsetting data.\")\n",
    "\n",
    "# Set up training and validation data\n",
    "X_train = train_data[['Latitude', 'Longitude', 'era5_uwnd', 'era5_vwnd']]\n",
    "y_train = train_data[['displacement', 'heading']]\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Set output file path\n",
    "output_file_path = f'../data/predictions/predictions_{val_buoy_id}.csv'\n",
    "\n",
    "# Ensure the predictions directory exists\n",
    "predictions_dir = os.path.dirname(output_file_path)\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.makedirs(predictions_dir)\n",
    "    print(f\"Directory {predictions_dir} created.\")\n",
    "else:\n",
    "    print(f\"Directory {predictions_dir} already exists.\")\n",
    "\n",
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs, output_file_path):\n",
    "    start_time = time.time()\n",
    "    max_duration = 4 * 60 * 60  # Maximum runtime in seconds\n",
    "\n",
    "    # Check if the output file exists and write the header only if it doesn't\n",
    "    if not os.path.exists(output_file_path):\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(\"Predicted_Latitude,Predicted_Longitude,Datetime,BuoyID\\n\")\n",
    "    \n",
    "    with open(output_file_path, 'a') as file:\n",
    "        current_lat, current_lon = val_subset.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = val_subset.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "        buoy_id = val_data.iloc[0]['BuoyID']\n",
    "        \n",
    "        print(\"\\nInitial conditions:\")\n",
    "        print(f\"Latitude: {current_lat:.2f}, Longitude: {current_lon:.2f}, Datetime: {val_data.iloc[0]['datetime']}, BuoyID: {buoy_id}\")\n",
    "\n",
    "        for i in range(1, len(val_data)):\n",
    "            # Check if the maximum duration has been exceeded\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > max_duration:\n",
    "                print(\"Maximum duration exceeded. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            next_row = val_data.iloc[i]\n",
    "            \n",
    "            input_data = pd.DataFrame({\n",
    "                'Latitude': [current_lat],\n",
    "                'Longitude': [current_lon],\n",
    "                'era5_uwnd': [current_uwnd],\n",
    "                'era5_vwnd': [current_vwnd]\n",
    "            })\n",
    "            \n",
    "            prediction_start_time = time.time()\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "            prediction_end_time = time.time()\n",
    "            print(f\"Prediction step {i} took {prediction_end_time - prediction_start_time:.2f} seconds.\")\n",
    "            \n",
    "            wind_extraction_start_time = time.time()\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "            wind_extraction_end_time = time.time()\n",
    "            print(f\"Wind extraction step {i} took {wind_extraction_end_time - wind_extraction_start_time:.2f} seconds.\")\n",
    "\n",
    "            print(f\"\\nPrediction for row {i}:\")\n",
    "            print(f\"Predicted Latitude: {predicted_lat:.2f}, Predicted Longitude: {predicted_lon:.2f}\")\n",
    "            print(f\"Predicted Displacement: {predicted_displacement:.2f}, Predicted Heading: {predicted_heading:.2f}\")\n",
    "            print(f\"Updated Wind U: {predicted_wind_u:.2f}, Wind V: {predicted_wind_v:.2f}\")\n",
    "            print(f\"Target Datetime: {next_row['datetime']}, BuoyID: {buoy_id}\")\n",
    "\n",
    "            # Write results to the file\n",
    "            file.write(f\"{round(predicted_lat, 3)},{round(predicted_lon, 3)},{next_row['datetime']},{buoy_id}\\n\")\n",
    "\n",
    "            # Stop the script if the file size exceeds 2 GB\n",
    "            file_size = file.tell()\n",
    "            if file_size > 2 * 1024 * 1024 * 1024:\n",
    "                print(\"File size exceeded 2 GB. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            # Update current state for next iteration\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "            gc.collect()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processed {i} predictions...\")\n",
    "\n",
    "    print(f\"\\nPrediction complete. Results saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "# Get unique BuoyIDs\n",
    "unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "\n",
    "# Iterate over each BuoyID in the validation data and make predictions\n",
    "for buoy_id in unique_buoy_ids:\n",
    "    # Set output file path\n",
    "    output_file_path = f'../data/predictions/predictions_{buoy_id}.csv'\n",
    "    \n",
    "    # Subset the data for the current BuoyID\n",
    "    buoy_data = val_data[val_data['BuoyID'] == buoy_id]\n",
    "\n",
    "    # Run iterative predictions on validation subset\n",
    "    print(\"\\nStarting iterative predictions on validation subset...\")\n",
    "    iterative_prediction(\n",
    "        val_data=buoy_data,\n",
    "        model=model,\n",
    "        tree=tree,\n",
    "        valid_times=valid_time_dt,\n",
    "        latitudes=latitudes,\n",
    "        longitudes=longitudes,\n",
    "        lat_lon_pairs=lat_lon_pairs,\n",
    "        output_file_path=output_file_path\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
