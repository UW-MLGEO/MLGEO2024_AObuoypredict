{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "from geopy import Point\n",
    "import math\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to pre-process spatial data and extract wind values from a lat/lon location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, uwnd_nc_file_path, vwnd_nc_file_path, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Load the NetCDF files\n",
    "        uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "        vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "        # Extract the wind values from the NetCDF files\n",
    "        uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "        vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../processed_buoy_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "buoy_data.head()\n",
    "\n",
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    \"\"\"\n",
    "    Calculate the new latitude and longitude given an initial position,\n",
    "    displacement in meters, and heading in degrees.\n",
    "    \n",
    "    Parameters:\n",
    "    - current_position: tuple (latitude, longitude) in decimal degrees\n",
    "    - displacement: distance to move in meters\n",
    "    - heading: angle in degrees from the north (0 degrees is north, 90 degrees is east)\n",
    "    \n",
    "    Returns:\n",
    "    - new_latitude, new_longitude: updated position in decimal degrees\n",
    "    \"\"\"\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    new_latitude = math.degrees(lat2)\n",
    "    new_longitude = math.degrees(lon2)\n",
    "    \n",
    "    return new_latitude, new_longitude\n",
    "\n",
    "print(\"Dropping unused columns...\")\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "print(\"Datetime column converted.\")\n",
    "\n",
    "print(\"Splitting data by BuoyID...\")\n",
    "buoy_ids = buoy_data['BuoyID'].unique()\n",
    "train_ids = np.random.choice(buoy_ids, size=int(len(buoy_ids) * 0.8), replace=False)\n",
    "val_ids = np.setdiff1d(buoy_ids, train_ids)\n",
    "\n",
    "train_data = buoy_data[buoy_data['BuoyID'].isin(train_ids)].copy()\n",
    "val_data = buoy_data[buoy_data['BuoyID'].isin(val_ids)].copy()\n",
    "\n",
    "print(\"Subsetting training and validation data for specific BuoyIDs...\")\n",
    "#CHANGE SIZE TO SUBSET DATA FOR TRAINING\n",
    "train_buoy_ids = np.random.choice(train_data['BuoyID'].unique(), size=105, replace=False)\n",
    "train_subset = train_data[train_data['BuoyID'].isin(train_buoy_ids)].copy()\n",
    "#CHANGE SIZE TO SUBSET DATA FOR VALIDATION\n",
    "val_buoy_id = np.random.choice(val_data['BuoyID'].unique(), size=1)[0]\n",
    "val_subset = val_data[val_data['BuoyID'] == val_buoy_id].copy()\n",
    "\n",
    "del train_data, val_data\n",
    "gc.collect()\n",
    "print(\"Memory cleaned up after subsetting data.\")\n",
    "\n",
    "X_train = train_subset[['Latitude', 'Longitude', 'era5_uwnd', 'era5_vwnd']]\n",
    "y_train = train_subset[['displacement', 'heading']]\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "output_file_path = f'../predictions_{val_buoy_id}.csv'\n",
    "\n",
    "# Check if the output file exists and delete it if it does\n",
    "if os.path.exists(output_file_path):\n",
    "    os.remove(output_file_path)\n",
    "    print(f\"Existing file {output_file_path} deleted.\")\n",
    "else:\n",
    "    print(f\"No existing file found at {output_file_path}.\")\n",
    "\n",
    "def iterative_prediction(val_subset, model, kdtree, uwnd_nc_file_path, vwnd_nc_file_path, valid_times, latitudes, longitudes, lat_lon_pairs, output_file_path):\n",
    "    # Check if the output file exists and write the header only if it doesn't\n",
    "    if not os.path.exists(output_file_path):\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(\"Predicted_Latitude,Predicted_Longitude,Datetime,BuoyID\\n\")\n",
    "    \n",
    "    with open(output_file_path, 'a') as file:\n",
    "        current_lat, current_lon = val_subset.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = val_subset.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "        buoy_id = val_subset.iloc[0]['BuoyID']\n",
    "        \n",
    "        print(\"\\nInitial conditions:\")\n",
    "        print(f\"Latitude: {current_lat:.2f}, Longitude: {current_lon:.2f}, Datetime: {val_subset.iloc[0]['datetime']}, BuoyID: {buoy_id}\")\n",
    "\n",
    "        for i in range(1, len(val_subset)):\n",
    "            next_row = val_subset.iloc[i]\n",
    "            \n",
    "            input_data = pd.DataFrame({\n",
    "                'Latitude': [current_lat],\n",
    "                'Longitude': [current_lon],\n",
    "                'era5_uwnd': [current_uwnd],\n",
    "                'era5_vwnd': [current_vwnd]\n",
    "            })\n",
    "            \n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                uwnd_nc_file_path,\n",
    "                vwnd_nc_file_path,\n",
    "                kdtree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs  # Pass lat_lon_pairs here\n",
    "            )\n",
    "\n",
    "            print(f\"\\nPrediction for row {i}:\")\n",
    "            print(f\"Predicted Latitude: {predicted_lat:.2f}, Predicted Longitude: {predicted_lon:.2f}\")\n",
    "            print(f\"Predicted Displacement: {predicted_displacement:.2f}, Predicted Heading: {predicted_heading:.2f}\")\n",
    "            print(f\"Updated Wind U: {predicted_wind_u:.2f}, Wind V: {predicted_wind_v:.2f}\")\n",
    "            print(f\"Target Datetime: {next_row['datetime']}, BuoyID: {buoy_id}\")\n",
    "\n",
    "            # Round numerical values before writing to the file\n",
    "            file.write(f\"{round(predicted_lat, 2)},{round(predicted_lon, 2)},{next_row['datetime']},{buoy_id}\\n\")\n",
    "\n",
    "            file_size = file.tell()\n",
    "            if file_size > 2 * 1024 * 1024 * 1024:\n",
    "                print(\"File size exceeded 2 GB. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "            gc.collect()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processed {i} predictions...\")\n",
    "    \n",
    "    print(f\"\\nPrediction complete. Results saved to {output_file_path}\")\n",
    "\n",
    "print(\"\\nStarting iterative predictions on validation subset...\")\n",
    "iterative_prediction(\n",
    "    val_subset=val_subset,\n",
    "    model=model,\n",
    "    kdtree=tree,\n",
    "    uwnd_nc_file_path=uwnd_nc_file_path,\n",
    "    vwnd_nc_file_path=vwnd_nc_file_path,\n",
    "    valid_times=valid_time_dt,\n",
    "    latitudes=latitudes,\n",
    "    longitudes=longitudes,\n",
    "    output_file_path=output_file_path,\n",
    "    lat_lon_pairs=lat_lon_pairs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
