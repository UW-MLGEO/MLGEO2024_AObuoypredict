{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso, BayesianRidge, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing KDTree and time differences...\n",
      "Error precomputing KDTree and time differences: [Errno 2] No such file or directory: b'../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
      "Error precomputing KDTree and time differences: [Errno 2] No such file or directory: b'../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m vwnd_nc_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs \u001b[38;5;241m=\u001b[39m precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError precomputing KDTree and time differences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m, in \u001b[0;36mprecompute_kdtree_and_time_diffs\u001b[1;34m(uwnd_nc_file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputing KDTree and time differences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the NetCDF file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m ds \u001b[38;5;241m=\u001b[39m nc\u001b[38;5;241m.\u001b[39mDataset(uwnd_nc_file_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Extract the valid_time, latitudes, and longitudes from the NetCDF file\u001b[39;00m\n\u001b[0;32m      9\u001b[0m valid_time \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_time\u001b[39m\u001b[38;5;124m'\u001b[39m][:]  \u001b[38;5;66;03m# Assuming 'valid_time' is the variable name for time\u001b[39;00m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'"
     ]
    }
   ],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    return math.degrees(lat2), math.degrees(lon2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs, output_file_path, max_predicts=None):\n",
    "\n",
    "    # Check if the output file exists and write the header only if it doesn't\n",
    "    if not os.path.exists(output_file_path):\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(\"Latitude,Longitude,Datetime,BuoyID,era5_uwnd,era5_vwnd,displacement,heading\\n\")\n",
    "\n",
    "    # Print the input data length\n",
    "    print(f\"Number of rows in input validation data: {len(val_data)}\")\n",
    "\n",
    "    with open(output_file_path, 'a') as file:\n",
    "        # Initialize starting conditions\n",
    "        current_lat, current_lon = val_data.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = val_data.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "        buoy_id = val_data.iloc[0]['BuoyID']\n",
    "        previous_time = val_data.iloc[0]['datetime']\n",
    "        \n",
    "        print(\"\\nInitial conditions:\")\n",
    "        print(f\"Latitude: {current_lat:.2f}, Longitude: {current_lon:.2f}, Datetime: {previous_time}, BuoyID: {buoy_id}\")\n",
    "\n",
    "        # Initialize output count for tracking\n",
    "        output_count = 0\n",
    "\n",
    "        for i in range(1, len(val_data)):  # Start from the second row for time_step calculation\n",
    "            # Check if the maximum number of predictions has been reached\n",
    "            if max_predicts is not None and i >= max_predicts:\n",
    "                print(f\"Maximum number of predictions ({max_predicts}) reached. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            next_row = val_data.iloc[i]\n",
    "            \n",
    "            # Calculate time_step between current and next row\n",
    "            time_step = (next_row['datetime'] - previous_time).total_seconds()\n",
    "            previous_time = next_row['datetime']  # Update previous_time for the next iteration\n",
    "            \n",
    "            # Convert wind components to magnitude and angle\n",
    "            wind_magnitude = (current_uwnd**2 + current_vwnd**2) ** 0.5\n",
    "            wind_angle = np.degrees(np.arctan2(current_vwnd, current_uwnd)) % 360  # Calculate wind angle in degrees\n",
    "            \n",
    "            # Prepare the input data for the model, including time_step\n",
    "            input_data = pd.DataFrame({\n",
    "                'wind_magnitude': [wind_magnitude],\n",
    "                'wind_angle': [wind_angle],\n",
    "                'time_step': [time_step]\n",
    "            })\n",
    "            \n",
    "            # Predict the displacement and heading\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "            \n",
    "            # Extract wind components for the predicted location and next datetime\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "            \n",
    "            # Update current position and wind components for the next prediction\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "            # Write the prediction to the output file with all columns in val_data\n",
    "            file.write(\n",
    "                f\"{current_lat},{current_lon},{next_row['datetime']},{buoy_id},{predicted_wind_u},{predicted_wind_v},{predicted_displacement},{predicted_heading}\\n\"\n",
    "            )\n",
    "            output_count += 1  # Increment output count for each successful prediction\n",
    "\n",
    "        # Print the output data length after predictions\n",
    "        print(f\"Number of rows written to output file: {output_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unused columns and retaining only: ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'wind_magnitude', 'wind_angle', 'displacement', 'heading']\n",
      "Datetime column successfully converted to datetime format.\n",
      "Calculating time step between successive observations.\n",
      "Splitting data into training and validation sets by unique Buoy IDs.\n",
      "Training IDs count: 281, Validation IDs count: 5\n",
      "Training data shape: (1761685, 9), Validation data shape: (17015, 9)\n",
      "Prepared training data with time step as additional feature. Feature set shape: (1761685, 5), Target set shape: (1761685, 2)\n"
     ]
    }
   ],
   "source": [
    "# Keep relevant columns for data preparation\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'wind_magnitude', 'wind_angle', 'displacement', 'heading']\n",
    "print(\"Dropping unused columns and retaining only:\", columns_to_keep)\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "print(\"Datetime column successfully converted to datetime format.\")\n",
    "\n",
    "# Calculate time step between successive rows for each buoy\n",
    "print(\"Calculating time step between successive observations.\")\n",
    "buoy_data = buoy_data.sort_values(by=['BuoyID', 'datetime']).reset_index(drop=True)\n",
    "buoy_data['time_step'] = buoy_data.groupby('BuoyID')['datetime'].diff().dt.total_seconds()\n",
    "buoy_data['time_step'].fillna(0, inplace=True)  # Fill NaN values in the first row of each buoy with 0\n",
    "\n",
    "# Split data by BuoyID into training and validation sets\n",
    "print(\"Splitting data into training and validation sets by unique Buoy IDs.\")\n",
    "buoy_ids = buoy_data['BuoyID'].unique()\n",
    "train_ids = np.random.choice(buoy_ids, size=int(len(buoy_ids) - 5), replace=False)\n",
    "val_ids = np.setdiff1d(buoy_ids, train_ids)\n",
    "print(f\"Training IDs count: {len(train_ids)}, Validation IDs count: {len(val_ids)}\")\n",
    "\n",
    "# Separate train and validation data\n",
    "train_data = buoy_data[buoy_data['BuoyID'].isin(train_ids)].copy()\n",
    "val_data = buoy_data[buoy_data['BuoyID'].isin(val_ids)].copy()\n",
    "print(f\"Training data shape: {train_data.shape}, Validation data shape: {val_data.shape}\")\n",
    "\n",
    "# Drop columns not used in training features after splitting\n",
    "X_train = train_data.drop(columns=['Latitude', 'Longitude', 'BuoyID', 'displacement', 'heading'])\n",
    "y_train = train_data[['displacement', 'heading']]\n",
    "\n",
    "X_val = val_data.drop(columns=['Latitude', 'Longitude', 'BuoyID', 'displacement', 'heading'])\n",
    "y_val = val_data[['displacement', 'heading']]\n",
    "\n",
    "print(f\"Prepared training data without displacement and heading as features. Feature set shape: {X_train.shape}, Target set shape: {y_train.shape}\")\n",
    "print(f\"Prepared validation data without displacement and heading as features. Feature set shape: {X_val.shape}, Target set shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wind_magnitude</th>\n",
       "      <th>wind_angle</th>\n",
       "      <th>displacement</th>\n",
       "      <th>heading</th>\n",
       "      <th>time_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.606201</td>\n",
       "      <td>-15.371969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.919853</td>\n",
       "      <td>-14.877097</td>\n",
       "      <td>205.312977</td>\n",
       "      <td>314.323130</td>\n",
       "      <td>1801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.919853</td>\n",
       "      <td>-14.877097</td>\n",
       "      <td>206.856929</td>\n",
       "      <td>314.768180</td>\n",
       "      <td>1830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.451795</td>\n",
       "      <td>-17.511613</td>\n",
       "      <td>208.707315</td>\n",
       "      <td>311.761871</td>\n",
       "      <td>1786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.451795</td>\n",
       "      <td>-17.511613</td>\n",
       "      <td>197.532749</td>\n",
       "      <td>305.046983</td>\n",
       "      <td>1784.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wind_magnitude  wind_angle  displacement     heading  time_step\n",
       "0       51.606201  -15.371969      0.000000    0.000000        0.0\n",
       "1       48.919853  -14.877097    205.312977  314.323130     1801.0\n",
       "2       48.919853  -14.877097    206.856929  314.768180     1830.0\n",
       "3       49.451795  -17.511613    208.707315  311.761871     1786.0\n",
       "4       49.451795  -17.511613    197.532749  305.046983     1784.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting iterative predictions on validation subset...\")\n",
    "\n",
    "output_file_path = '../data/processed/predictions/'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "# Get unique buoyID values in the validation data\n",
    "unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "\n",
    "# Loop through each buoyID and make predictions\n",
    "for buoy_id in unique_buoy_ids:\n",
    "    print(f\"Processing BuoyID: {buoy_id}\")\n",
    "    \n",
    "    # Filter validation data for the current buoyID\n",
    "    val_data_buoy = val_data[val_data['BuoyID'] == buoy_id]\n",
    "    \n",
    "    # Create an output file path for the current buoyID\n",
    "    output_file_path_buoy = f\"{output_file_path}predicted_{buoy_id}.csv\"\n",
    "    \n",
    "    # Print the output file path for the current buoyID\n",
    "    print(f\"Output file path for BuoyID {buoy_id}: {output_file_path_buoy}\")\n",
    "\n",
    "    # Make iterative predictions for the current buoyID\n",
    "    iterative_prediction(\n",
    "        val_data=val_data_buoy,\n",
    "        model=model,\n",
    "        tree=tree,\n",
    "        valid_times=valid_time_dt,\n",
    "        latitudes=latitudes,\n",
    "        longitudes=longitudes,\n",
    "        output_file_path=output_file_path_buoy,\n",
    "        lat_lon_pairs=lat_lon_pairs,\n",
    "        max_predicts=None\n",
    "    )\n",
    "    print(f\"Predictions for BuoyID {buoy_id} completed and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlggeo2024_aobuoypredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
