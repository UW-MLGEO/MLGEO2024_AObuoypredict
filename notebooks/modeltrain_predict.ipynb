{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso, BayesianRidge, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing KDTree and time differences...\n",
      "KDTree and time differences precomputed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    return math.degrees(lat2), math.degrees(lon2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs, output_file_path, max_predicts=None):\n",
    "\n",
    "    # Check if the output file exists and write the header only if it doesn't\n",
    "    if not os.path.exists(output_file_path):\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(\"Latitude,Longitude,Datetime,BuoyID,era5_uwnd,era5_vwnd,displacement,heading\\n\")\n",
    "\n",
    "    # Print the input data length\n",
    "    print(f\"Number of rows in input validation data: {len(val_data)}\")\n",
    "\n",
    "    with open(output_file_path, 'a') as file:\n",
    "        # Initialize starting conditions\n",
    "        current_lat, current_lon = val_data.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = val_data.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "        buoy_id = val_data.iloc[0]['BuoyID']\n",
    "        previous_time = val_data.iloc[0]['datetime']\n",
    "        \n",
    "        print(\"\\nInitial conditions:\")\n",
    "        print(f\"Latitude: {current_lat:.2f}, Longitude: {current_lon:.2f}, Datetime: {previous_time}, BuoyID: {buoy_id}\")\n",
    "\n",
    "        # Initialize output count for tracking\n",
    "        output_count = 0\n",
    "\n",
    "        for i in range(1, len(val_data)):  # Start from the second row for time_step calculation\n",
    "            # Check if the maximum number of predictions has been reached\n",
    "            if max_predicts is not None and i >= max_predicts:\n",
    "                print(f\"Maximum number of predictions ({max_predicts}) reached. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            next_row = val_data.iloc[i]\n",
    "            \n",
    "            # Calculate time_step between current and next row\n",
    "            time_step = (next_row['datetime'] - previous_time).total_seconds()\n",
    "            previous_time = next_row['datetime']  # Update previous_time for the next iteration\n",
    "            \n",
    "            # Convert wind components to magnitude and angle\n",
    "            wind_magnitude = (current_uwnd**2 + current_vwnd**2) ** 0.5\n",
    "            wind_angle = np.degrees(np.arctan2(current_vwnd, current_uwnd)) % 360  # Calculate wind angle in degrees\n",
    "            \n",
    "            # Prepare the input data for the model, including time_step\n",
    "            input_data = pd.DataFrame({\n",
    "                'wind_magnitude': [wind_magnitude],\n",
    "                'wind_angle': [wind_angle],\n",
    "                'time_step': [time_step]\n",
    "            })\n",
    "            \n",
    "            # Predict the displacement and heading\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "            \n",
    "            # Extract wind components for the predicted location and next datetime\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "            \n",
    "            # Update current position and wind components for the next prediction\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "            # Write the prediction to the output file with all columns in val_data\n",
    "            file.write(\n",
    "                f\"{current_lat},{current_lon},{next_row['datetime']},{buoy_id},{predicted_wind_u},{predicted_wind_v},{predicted_displacement},{predicted_heading}\\n\"\n",
    "            )\n",
    "            output_count += 1  # Increment output count for each successful prediction\n",
    "\n",
    "        # Print the output data length after predictions\n",
    "        print(f\"Number of rows written to output file: {output_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unused columns and retaining only: ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'wind_magnitude', 'wind_angle', 'displacement', 'heading', 'era5_uwnd', 'era5_vwnd']\n",
      "Datetime column successfully converted to datetime format.\n",
      "Calculating time step between successive observations.\n",
      "Splitting data into training and validation sets by unique Buoy IDs.\n",
      "Training IDs count: 281, Validation IDs count: 5\n",
      "Training data shape: (1729023, 11), Validation data shape: (49677, 11)\n",
      "Prepared training data without displacement and heading as features. Feature set shape: (1729023, 3), Target set shape: (1729023, 2)\n",
      "Prepared validation data without displacement and heading as features. Feature set shape: (49677, 3), Target set shape: (49677, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "buoy_data = pd.read_csv('../processed_buoy_data.csv')\n",
    "\n",
    "# Keep relevant columns for data preparation\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'wind_magnitude', 'wind_angle', \n",
    "                   'displacement', 'heading', 'era5_uwnd', 'era5_vwnd']\n",
    "print(\"Dropping unused columns and retaining only:\", columns_to_keep)\n",
    "buoy_data = buoy_data[columns_to_keep]\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "print(\"Datetime column successfully converted to datetime format.\")\n",
    "\n",
    "# Calculate time step between successive rows for each buoy\n",
    "print(\"Calculating time step between successive observations.\")\n",
    "buoy_data = buoy_data.sort_values(by=['BuoyID', 'datetime']).reset_index(drop=True)\n",
    "buoy_data['time_step'] = buoy_data.groupby('BuoyID')['datetime'].diff().dt.total_seconds()\n",
    "buoy_data['time_step'] = buoy_data['time_step'].fillna(0)  # Fill NaN values with 0\n",
    "\n",
    "# Split data by BuoyID into training and validation sets\n",
    "print(\"Splitting data into training and validation sets by unique Buoy IDs.\")\n",
    "buoy_ids = buoy_data['BuoyID'].unique()\n",
    "train_ids = np.random.choice(buoy_ids, size=int(len(buoy_ids) - 5), replace=False)\n",
    "val_ids = np.setdiff1d(buoy_ids, train_ids)\n",
    "print(f\"Training IDs count: {len(train_ids)}, Validation IDs count: {len(val_ids)}\")\n",
    "\n",
    "# Separate train and validation data\n",
    "train_data = buoy_data[buoy_data['BuoyID'].isin(train_ids)]\n",
    "val_data = buoy_data[buoy_data['BuoyID'].isin(val_ids)]\n",
    "print(f\"Training data shape: {train_data.shape}, Validation data shape: {val_data.shape}\")\n",
    "\n",
    "# Define columns to keep for features and target\n",
    "feature_columns = ['wind_magnitude', 'wind_angle', 'time_step']\n",
    "target_columns = ['displacement', 'heading']\n",
    "\n",
    "# Create training and validation feature and target sets\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_columns]\n",
    "\n",
    "X_val = val_data[feature_columns]\n",
    "y_val = val_data[target_columns]\n",
    "\n",
    "print(f\"Prepared training data with feature columns {feature_columns}. Feature set shape: {X_train.shape}, Target set shape: {y_train.shape}\")\n",
    "print(f\"Prepared validation data with feature columns {feature_columns}. Feature set shape: {X_val.shape}, Target set shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting iterative predictions on validation data...\n",
      "Processing BuoyID: 902002\n",
      "Output file path for BuoyID 902002: ../data/processed/predictions/predicted_902002.csv\n",
      "Number of rows in input validation data: 17521\n",
      "\n",
      "Initial conditions:\n",
      "Latitude: 84.07, Longitude: -115.32, Datetime: 2023-01-01 00:00:46, BuoyID: 902002\n",
      "Number of rows written to output file: 17520\n",
      "Predictions for BuoyID 902002 completed and saved\n",
      "Processing BuoyID: 300234066034140\n",
      "Output file path for BuoyID 300234066034140: ../data/processed/predictions/predicted_300234066034140.csv\n",
      "Number of rows in input validation data: 8829\n",
      "\n",
      "Initial conditions:\n",
      "Latitude: 79.49, Longitude: -7.59, Datetime: 2023-01-01 00:00:00, BuoyID: 300234066034140\n",
      "Number of rows written to output file: 8828\n",
      "Predictions for BuoyID 300234066034140 completed and saved\n",
      "Processing BuoyID: 300234067525560\n",
      "Output file path for BuoyID 300234067525560: ../data/processed/predictions/predicted_300234067525560.csv\n",
      "Number of rows in input validation data: 229\n",
      "\n",
      "Initial conditions:\n",
      "Latitude: 70.18, Longitude: -18.14, Datetime: 2023-01-01 00:00:26, BuoyID: 300234067525560\n",
      "Number of rows written to output file: 228\n",
      "Predictions for BuoyID 300234067525560 completed and saved\n",
      "Processing BuoyID: 300234068241260\n",
      "Output file path for BuoyID 300234068241260: ../data/processed/predictions/predicted_300234068241260.csv\n",
      "Number of rows in input validation data: 8345\n",
      "\n",
      "Initial conditions:\n",
      "Latitude: 47.93, Longitude: -45.39, Datetime: 2023-01-01 00:01:00, BuoyID: 300234068241260\n",
      "Number of rows written to output file: 8344\n",
      "Predictions for BuoyID 300234068241260 completed and saved\n",
      "Processing BuoyID: 300534063449630\n",
      "Output file path for BuoyID 300534063449630: ../data/processed/predictions/predicted_300534063449630.csv\n",
      "Number of rows in input validation data: 14753\n",
      "\n",
      "Initial conditions:\n",
      "Latitude: 70.85, Longitude: -143.07, Datetime: 2023-09-16 20:30:05, BuoyID: 300534063449630\n",
      "Number of rows written to output file: 14752\n",
      "Predictions for BuoyID 300534063449630 completed and saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting iterative predictions on validation data...\")\n",
    "\n",
    "output_file_path = '../data/processed/predictions/'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "# Get unique buoyID values in the validation data\n",
    "unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "\n",
    "# Loop through each buoyID and make predictions\n",
    "for buoy_id in unique_buoy_ids:\n",
    "    print(f\"Processing BuoyID: {buoy_id}\")\n",
    "    \n",
    "    # Filter validation data for the current buoyID\n",
    "    val_data_buoy = val_data[val_data['BuoyID'] == buoy_id]\n",
    "    \n",
    "    # Create an output file path for the current buoyID\n",
    "    output_file_path_buoy = f\"{output_file_path}predicted_{buoy_id}.csv\"\n",
    "    \n",
    "    # Print the output file path for the current buoyID\n",
    "    print(f\"Output file path for BuoyID {buoy_id}: {output_file_path_buoy}\")\n",
    "\n",
    "    # Run iterative prediction with error handling to track any potential issues\n",
    "    try:\n",
    "        iterative_prediction(\n",
    "            val_data=val_data_buoy,\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            output_file_path=output_file_path_buoy,\n",
    "            lat_lon_pairs=lat_lon_pairs,\n",
    "            max_predicts=None\n",
    "        )\n",
    "        print(f\"Predictions for BuoyID {buoy_id} completed and saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered for BuoyID {buoy_id}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlggeo2024_aobuoypredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
