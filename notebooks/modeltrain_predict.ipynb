{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso, BayesianRidge, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing KDTree and time differences...\n",
      "KDTree and time differences precomputed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    return math.degrees(lat2), math.degrees(lon2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs, output_file_path, max_predicts=None):\n",
    "\n",
    "    # Check if the output file exists and write the header only if it doesn't\n",
    "    if not os.path.exists(output_file_path):\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(\"Predicted_Latitude,Predicted_Longitude,Datetime,BuoyID,Wind_U,Wind_V\\n\")\n",
    "\n",
    "    # Print the input data length\n",
    "    print(f\"Number of rows in input validation data: {len(val_data)}\")\n",
    "\n",
    "    with open(output_file_path, 'a') as file:\n",
    "        current_lat, current_lon = val_data.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = val_data.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "        buoy_id = val_data.iloc[0]['BuoyID']\n",
    "        \n",
    "        print(\"\\nInitial conditions:\")\n",
    "        print(f\"Latitude: {current_lat:.2f}, Longitude: {current_lon:.2f}, Datetime: {val_data.iloc[0]['datetime']}, BuoyID: {buoy_id}\")\n",
    "\n",
    "        # Initialize output count for tracking\n",
    "        output_count = 0\n",
    "\n",
    "        for i in range(len(val_data)):\n",
    "\n",
    "            # Check if the maximum number of predictions has been reached\n",
    "            if max_predicts is not None and i >= max_predicts:\n",
    "                print(f\"Maximum number of predictions ({max_predicts}) reached. Stopping the script.\")\n",
    "                break\n",
    "\n",
    "            next_row = val_data.iloc[i]\n",
    "            \n",
    "            input_data = pd.DataFrame({\n",
    "                'Latitude': [current_lat],\n",
    "                'Longitude': [current_lon],\n",
    "                'era5_uwnd': [current_uwnd],\n",
    "                'era5_vwnd': [current_vwnd],\n",
    "                'BuoyID': [buoy_id]\n",
    "            })\n",
    "            \n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "            \n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "            \n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "            # Write the prediction to the output file, including wind components\n",
    "            file.write(f\"{current_lat},{current_lon},{next_row['datetime']},{buoy_id},{predicted_wind_u},{predicted_wind_v}\\n\")\n",
    "            output_count += 1  # Increment output count for each successful prediction\n",
    "\n",
    "        # Print the output data length after predictions\n",
    "        print(f\"Number of rows written to output file: {output_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping unused columns and retaining only: ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'wind_magnitude', 'wind_angle', 'displacement', 'heading']\n",
      "Datetime column successfully converted to datetime format.\n",
      "Splitting data into training and validation sets by unique Buoy IDs.\n",
      "Training IDs count: 281, Validation IDs count: 5\n",
      "Training data shape: (1742093, 8), Validation data shape: (36607, 8)\n",
      "Prepared training data with temporal windowing. Feature set shape: (1740688, 5), Target set shape: (1740688, 2)\n",
      "Training model...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for track handling\n",
    "temporal_window_size = 5  # Define the size of the window in time steps\n",
    "\n",
    "# Load and prepare data\n",
    "buoy_data = pd.read_csv('../processed_buoy_data.csv')\n",
    "\n",
    "# Keep relevant columns and convert datetime\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'wind_magnitude', 'wind_angle', 'displacement', 'heading']\n",
    "print(\"Dropping unused columns and retaining only:\", columns_to_keep)\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "print(\"Datetime column successfully converted to datetime format.\")\n",
    "\n",
    "# Split data by BuoyID into training and validation sets\n",
    "print(\"Splitting data into training and validation sets by unique Buoy IDs.\")\n",
    "buoy_ids = buoy_data['BuoyID'].unique()\n",
    "train_ids = np.random.choice(buoy_ids, size=int(len(buoy_ids) - 5), replace=False)\n",
    "val_ids = np.setdiff1d(buoy_ids, train_ids)\n",
    "print(f\"Training IDs count: {len(train_ids)}, Validation IDs count: {len(val_ids)}\")\n",
    "\n",
    "train_data = buoy_data[buoy_data['BuoyID'].isin(train_ids)].copy()\n",
    "val_data = buoy_data[buoy_data['BuoyID'].isin(val_ids)].copy()\n",
    "print(f\"Training data shape: {train_data.shape}, Validation data shape: {val_data.shape}\")\n",
    "\n",
    "# Prepare temporal windows for training\n",
    "X_train, y_train = [], []\n",
    "for buoy_id in train_data['BuoyID'].unique():\n",
    "    track_data = train_data[train_data['BuoyID'] == buoy_id].sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    for i in range(temporal_window_size, len(track_data)):\n",
    "        # Select data within the temporal window\n",
    "        window = track_data.iloc[i-temporal_window_size:i]\n",
    "        \n",
    "        # Aggregate window data (example: mean or last values)\n",
    "        window_features = [\n",
    "            window['Latitude'].values[-1],  # Current latitude\n",
    "            window['Longitude'].values[-1], # Current longitude\n",
    "            window['wind_magnitude'].values[-1], # Current wind magnitude\n",
    "            window['wind_angle'].values[-1],     # Current wind angle\n",
    "            buoy_id                               # Buoy ID (as-is or encoded if needed)\n",
    "        ]\n",
    "        \n",
    "        # Append features and target for each window\n",
    "        X_train.append(window_features)\n",
    "        y_train.append(track_data.iloc[i][['displacement', 'heading']].values)\n",
    "\n",
    "# Convert to DataFrame for model training\n",
    "X_train = pd.DataFrame(X_train, columns=['Latitude', 'Longitude', 'wind_magnitude', 'wind_angle', 'BuoyID'])\n",
    "y_train = pd.DataFrame(y_train, columns=['displacement', 'heading'])\n",
    "\n",
    "print(f\"Prepared training data with temporal windowing. Feature set shape: {X_train.shape}, Target set shape: {y_train.shape}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting iterative predictions on validation subset...\n",
      "Processing BuoyID: 900126\n",
      "Output file path for BuoyID 900126: ../data/processed/predictions/predicted_900126.csv\n",
      "Number of rows in input validation data: 17433\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['era5_uwnd', 'era5_vwnd'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput file path for BuoyID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuoy_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file_path_buoy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Make iterative predictions for the current buoyID\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m iterative_prediction(\n\u001b[0;32m     27\u001b[0m     val_data\u001b[38;5;241m=\u001b[39mval_data_buoy,\n\u001b[0;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     29\u001b[0m     tree\u001b[38;5;241m=\u001b[39mtree,\n\u001b[0;32m     30\u001b[0m     valid_times\u001b[38;5;241m=\u001b[39mvalid_time_dt,\n\u001b[0;32m     31\u001b[0m     latitudes\u001b[38;5;241m=\u001b[39mlatitudes,\n\u001b[0;32m     32\u001b[0m     longitudes\u001b[38;5;241m=\u001b[39mlongitudes,\n\u001b[0;32m     33\u001b[0m     output_file_path\u001b[38;5;241m=\u001b[39moutput_file_path_buoy,\n\u001b[0;32m     34\u001b[0m     lat_lon_pairs\u001b[38;5;241m=\u001b[39mlat_lon_pairs,\n\u001b[0;32m     35\u001b[0m     max_predicts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions for BuoyID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbuoy_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed and saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36miterative_prediction\u001b[1;34m(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs, output_file_path, max_predicts)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     12\u001b[0m     current_lat, current_lon \u001b[38;5;241m=\u001b[39m val_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 13\u001b[0m     current_uwnd, current_vwnd \u001b[38;5;241m=\u001b[39m val_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mera5_uwnd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mera5_vwnd\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     14\u001b[0m     buoy_id \u001b[38;5;241m=\u001b[39m val_data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuoyID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInitial conditions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1072\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[1;32m-> 1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_with(key)\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1113\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[1;32m-> 1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benco\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['era5_uwnd', 'era5_vwnd'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting iterative predictions on validation subset...\")\n",
    "\n",
    "output_file_path = '../data/processed/predictions/'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "# Get unique buoyID values in the validation data\n",
    "unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "\n",
    "# Loop through each buoyID and make predictions\n",
    "for buoy_id in unique_buoy_ids:\n",
    "    print(f\"Processing BuoyID: {buoy_id}\")\n",
    "    \n",
    "    # Filter validation data for the current buoyID\n",
    "    val_data_buoy = val_data[val_data['BuoyID'] == buoy_id]\n",
    "    \n",
    "    # Create an output file path for the current buoyID\n",
    "    output_file_path_buoy = f\"{output_file_path}predicted_{buoy_id}.csv\"\n",
    "    \n",
    "    # Print the output file path for the current buoyID\n",
    "    print(f\"Output file path for BuoyID {buoy_id}: {output_file_path_buoy}\")\n",
    "\n",
    "    # Make iterative predictions for the current buoyID\n",
    "    iterative_prediction(\n",
    "        val_data=val_data_buoy,\n",
    "        model=model,\n",
    "        tree=tree,\n",
    "        valid_times=valid_time_dt,\n",
    "        latitudes=latitudes,\n",
    "        longitudes=longitudes,\n",
    "        output_file_path=output_file_path_buoy,\n",
    "        lat_lon_pairs=lat_lon_pairs,\n",
    "        max_predicts=None\n",
    "    )\n",
    "    print(f\"Predictions for BuoyID {buoy_id} completed and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
